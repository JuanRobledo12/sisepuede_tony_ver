{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "255f4f1b-ba3c-485d-967d-67d57b6e7499",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/model_attributes.py:2515: UserWarning: Invalid subsector attribute 'key_varreqs_partial'. Valid return type values are:'pycategory_primary', 'abv_subsector', 'sector', 'abv_sector', 'key_varreqs_all'\n",
      "  warnings.warn(f\"Invalid subsector attribute '{return_type}'. Valid return type values are:{valid_rts}\")\n",
      "/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/model_attributes.py:2272: FutureWarning: In a future version of pandas, a length 1 tuple will be returned when iterating over a groupby with a grouper equal to a list of length 1. Don't supply a list with a single grouper to avoid this warning.\n",
      "  for desig, df in df_by_designation:\n"
     ]
    }
   ],
   "source": [
    "from attribute_table import AttributeTable\n",
    "import importlib\n",
    "import ingestion as ing\n",
    "import inspect\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "#from model_attributes import *\n",
    "import model_attributes as ma\n",
    "import model_afolu as mafl\n",
    "import model_ippu as mi\n",
    "import model_circular_economy as mc\n",
    "import model_electricity as ml\n",
    "import model_energy as me\n",
    "import model_socioeconomic as se\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "import pandas as pd\n",
    "import re\n",
    "import setup_analysis as sa\n",
    "import sisepuede_data_api as api\n",
    "import sisepuede_models as sm\n",
    "import support_classes as sc\n",
    "import support_functions as sf\n",
    "import time\n",
    "from typing import *\n",
    "import warnings\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a63f81-fda7-48dd-9c9f-efc44ba2b0ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bbf53173-3994-413c-91c0-3a37fb9be221",
   "metadata": {},
   "source": [
    "###  Get key datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 905,
   "id": "7cc26c24-bc67-4115-934f-9e89233c1087",
   "metadata": {},
   "outputs": [],
   "source": [
    "##  COMPONENTS FOR READING TABLES\n",
    "importlib.reload(sc)\n",
    "\n",
    "\n",
    "# some directories \n",
    "dir_data_afolu = \"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/AFOLU/\"\n",
    "dir_repo_data = \"/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data\"\n",
    "\n",
    "\n",
    "# file paths\n",
    "\n",
    "fp_attr_luc = os.path.join(dir_data, \"attribute_fao_land_use_category_glcshare.csv\")\n",
    "fp_ipcc_forest = os.path.join(sa.dir_ref, \"data_tables_and_derivations\", \"AFOLU\", \"ipcc_afolu_c4_forested_land_tables.xlsx\")\n",
    "fp_ipcc_grassland = os.path.join(sa.dir_ref, \"data_tables_and_derivations\", \"AFOLU\", \"ipcc_afolu_c6_grassland_tables.xlsx\")\n",
    "fp_kcc_cw = os.path.join(dir_data_afolu, \"attribute_kcc.csv\")\n",
    "\n",
    "\n",
    "# some shared fields\n",
    "\n",
    "field_cats_sisepuede = \"sisepuede_categories\"\n",
    "field_continent = \"continent\"\n",
    "field_count = \"count\"\n",
    "field_country = \"Country\"\n",
    "field_crop_area = \"crop_area_ha\"\n",
    "field_crop_area_ca = \"crop_area_ha_cons_agrc\"\n",
    "field_domain = \"domain\"\n",
    "field_ecological_zone = \"ecological_zone1\"\n",
    "field_frac_ca = \"frac_crops_cons_agrc\"\n",
    "field_growth_natural = \"biomass_net_growth_natural_dm_tonnes_per_ha_per_year\"\n",
    "field_growth_plantation = \"biomass_net_growth_plantation_dm_tonnes_per_ha_per_year\"\n",
    "field_kcc = \"kcc\"\n",
    "field_luc = \"luc\"\n",
    "field_luc_attr_name = \"land_use_category_name\"\n",
    "field_region_kassam = \"region_kasssam\"\n",
    "field_storage_grassland = \"biomass_storage_grasslands_dm_tonnes_per_ha\"\n",
    "field_storage_grassland_total = \"biomass_storage_above_and_below_grasslands_dm_tonnes_per_ha\"\n",
    "field_storage_natural = \"biomass_storage_forest_natural_dm_tonnes_per_ha\"\n",
    "field_storage_plantation = \"biomass_storage_forest_plantation_dm_tonnes_per_ha\"\n",
    "field_type_factor = \"factor_type\"\n",
    "field_type_forest = \"forest_type\"\n",
    "field_type_forest_ipcc = \"ipcc_forest\"\n",
    "\n",
    "\n",
    "# some derivative classes\n",
    "\n",
    "model_afolu = mafl.AFOLU(sa.model_attributes)\n",
    "model_socioeconomic = model_afolu.model_socioeconomic\n",
    "\n",
    "regions = sc.Regions(sa.model_attributes)\n",
    "time_periods = sc.TimePeriods(sa.model_attributes)\n",
    "repo = api.SISEPUEDEBatchDataRepository(\n",
    "    dir_repo_data,\n",
    "    sa.model_attributes\n",
    ")\n",
    "\n",
    "attr_agrc = sa.model_attributes.get_attribute_table(f\"{sa.model_attributes.subsec_name_agrc}\")\n",
    "attr_frst = sa.model_attributes.get_attribute_table(f\"{sa.model_attributes.subsec_name_frst}\")\n",
    "attr_lndu = sa.model_attributes.get_attribute_table(f\"{sa.model_attributes.subsec_name_lndu}\")\n",
    "\n",
    "\n",
    "# some lists\n",
    "\n",
    "years_hist = list(range(2010, 2021))\n",
    "years_proj = [x for x in time_periods.all_years if x not in years_hist]\n",
    "\n",
    "\n",
    "\n",
    "####################################\n",
    "#    DEFINE SOME DATA FUNCTIONS    #\n",
    "####################################\n",
    "\n",
    "def build_kcc_luc_agg_file(\n",
    "    dir_read: str,\n",
    "    regex_match: re.Pattern,\n",
    "    model_attributes: ma.ModelAttributes,\n",
    "    field_types: Union[Dict[str, str], None] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build an aggregate data from of KCC and land use classification (LUC) by\n",
    "        ISO code\n",
    "    \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - dir_read: directory containing the files to concatenate\n",
    "    - regex_match: regex for files containing the kcc/luc indices by ISO code\n",
    "    - model_attributes: model attributes to use for assigning ISO field\n",
    "    \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - field_types: optional map of string to Pandas data type to apply for fields\n",
    "    \"\"\"\n",
    "    \n",
    "    fls_read = sorted([x for x in os.listdir(dir_read) if regex_match.match(x) is not None])\n",
    "    regions = sc.Regions(model_attributes)\n",
    "    field_iso = regions.field_iso\n",
    "    \n",
    "    df_out = []\n",
    "\n",
    "    for i, fl in enumerate(fls_read):\n",
    "        \n",
    "        # get iso code\n",
    "        iso = regex_match.match(fl)\n",
    "        iso = iso.groups()[0]\n",
    "        \n",
    "        fp_cur = os.path.join(dir_read, fl)\n",
    "        df_cur = pd.read_csv(fp_cur)\n",
    "        df_cur[field_iso] = iso\n",
    "        \n",
    "        if isinstance(field_types, dict):\n",
    "            for field, tp in field_types.items():\n",
    "                try:\n",
    "                    df_cur[field] = (\n",
    "                        df_cur[field].astype(tp)\n",
    "                        if field in df_cur.columns\n",
    "                        else df_cur[field]\n",
    "                    )\n",
    "                except:\n",
    "                    continue\n",
    "            \n",
    "        df_out.append(df_cur)\n",
    "        \n",
    "        \n",
    "    df_out = pd.concat(df_out, axis = 0)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def get_carbon_factors_forest(\n",
    "    fp_ipcc_forest: str,\n",
    "    sheet_name: str = \"table 4.12_2019R\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in IPCC V4 Table 4.12 from Excel and clean\n",
    "    \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - fp_ipcc_forest: path to Excel file\n",
    "        \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - sheet_name: sheet name in fp_ipcc_forest\n",
    "    \"\"\"\n",
    "    # read and clean\n",
    "    df_carbon_factors = pd.read_excel(fp_ipcc_forest, sheet_name = sheet_name)\n",
    "    df_carbon_factors = sf.clean_field_names(df_carbon_factors)\n",
    "    \n",
    "    # rename\n",
    "    dict_rnm = {\n",
    "        \"above__ground_biomass_in_natural_forests_(tonnes_d_m__ha_1)\": field_storage_natural,\n",
    "        \"above__ground_biomass_in_forest_plantation_s_(tonnes_d_m__ha_1)\": field_storage_plantation,\n",
    "        \"above__ground_net_biomass_growth_in_natural_forests_(tonnes_d_m__ha_1_yr_1)\": field_growth_natural,\n",
    "        \"above__ground_net_biomass_growth_in_forest_plantations_(tonnes_d_m__ha_1_yr_1)\": field_growth_plantation,\n",
    "        \"status/\\ncondition\": field_type_forest,\n",
    "    }\n",
    "    df_carbon_factors.rename(columns = dict_rnm, inplace = True)\n",
    "    \n",
    "    \n",
    "    # clean fields\n",
    "    fields_clean = [\n",
    "        field_storage_natural,\n",
    "        field_storage_plantation,\n",
    "        field_growth_natural,\n",
    "        field_growth_plantation\n",
    "    ]\n",
    "    for field in fields_clean:\n",
    "        vec = list(df_carbon_factors[field])\n",
    "        for i in range(len(df_carbon_factors)):\n",
    "            val = df_carbon_factors[field].iloc[i]\n",
    "            \n",
    "            if sf.isnumber(val):\n",
    "                val = val\n",
    "            elif isinstance(val, str):\n",
    "                # check if range is specified\n",
    "                if \"-\" in val:\n",
    "                    vals = val.split(\"-\")\n",
    "                    val = np.mean([float(x) for x in vals])\n",
    "                    \n",
    "                else:\n",
    "                    try:\n",
    "                        x = float(val)\n",
    "                    except:\n",
    "                        x = np.nan\n",
    "                    val = x\n",
    "            \n",
    "            vec[i] = val\n",
    "            \n",
    "        df_carbon_factors[field] = vec     \n",
    "    \n",
    "    \n",
    "    # clean\n",
    "    return df_carbon_factors\n",
    "\n",
    "\n",
    "\n",
    "def get_carbon_factors_grassland(\n",
    "    fp_ipcc_grassland: str,\n",
    "    field_kcc: str = field_kcc,\n",
    "    field_zone: str = \"ipcc_climate_zone\",\n",
    "    sheet_name: str = \"table 6.4_2006\",\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read in IPCC V4 Table 6.4 from Excel and clean\n",
    "    \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - fp_ipcc_grassland: path to Excel file\n",
    "        \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - field_kcc: field storing kcc code\n",
    "    - field_zone: field storing climate zone in input data frame (cleaned)\n",
    "    - sheet_name: sheet name in fp_ipcc_grassland\n",
    "    \"\"\"\n",
    "    # read and clean\n",
    "    df_carbon_factors = pd.read_excel(fp_ipcc_grassland, sheet_name = sheet_name)\n",
    "    df_carbon_factors = sf.clean_field_names(df_carbon_factors)\n",
    "    (\n",
    "        df_carbon_factors\n",
    "        .drop(\n",
    "            [x for x in df_carbon_factors.columns if \"error\" in x],\n",
    "            axis = 1,\n",
    "            inplace = True,\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # dictionary to map grass climates to kopen climate (manual)\n",
    "    dict_ipcc_grass_climates_to_kcc = {\n",
    "        \"Boreal - Dry & Wet\": [411, 412, 413, 414, 421, 422, 423, 424, 431, 432, 433, 434],\n",
    "        \"Cold Temperate - Dry\": [212, 222, 313, 323, 510, 520],\n",
    "        \"Warm Temperate - Dry\": [211, 221, 311, 312, 321, 322],\n",
    "        \"Cold Temperate - Wet\": [333],\n",
    "        \"Warm Temperate - Wet\": [331, 332],\n",
    "        # tropical distinctions: https://en.wikipedia.org/wiki/Tropical_climate\n",
    "        \"Tropical - Wet\": [110, 130],\n",
    "        \"Tropical - Dry\": [120]\n",
    "    }\n",
    "\n",
    "    # rename\n",
    "    dict_rnm = {\n",
    "        \"peak_above_ground_biomass_(tonnes_d_m__ha_1)\": field_storage_grassland,\n",
    "        \"total_(above_ground_and_below_ground)_non_woody_biomass\": field_storage_grassland_total,\n",
    "\n",
    "    }\n",
    "    df_carbon_factors.rename(columns = dict_rnm, inplace = True)\n",
    "    \n",
    "    # build climate zones domain data frame\n",
    "    dict_cc_to_zone = dict(\n",
    "        sum(\n",
    "            [\n",
    "                list(zip(v, [k for x in v])) \n",
    "                for k, v in dict_ipcc_grass_climates_to_kcc.items()\n",
    "            ],\n",
    "            []\n",
    "        )\n",
    "    )\n",
    "    df_kcc = pd.DataFrame({field_kcc: list(dict_cc_to_zone.keys())})\n",
    "    df_kcc[field_zone] = df_kcc[field_kcc].replace(dict_cc_to_zone)\n",
    "    \n",
    "    # merge in data and drop field\n",
    "    df_carbon_factors = (\n",
    "        pd.merge(\n",
    "            df_kcc, \n",
    "            df_carbon_factors,\n",
    "            how = \"left\"\n",
    "        )\n",
    "        .drop([field_zone], axis = 1)\n",
    "    ) \n",
    "    \n",
    "    return df_carbon_factors\n",
    "\n",
    "\n",
    "\n",
    "def get_climate_data(\n",
    "    fp_climate_counts: str,\n",
    "    fp_kcc_attribute: str,\n",
    "    field_codenum_kcc: str = \"code_num\",\n",
    "    field_iso_climate: str = \"ISO_A3\",\n",
    "    field_key_kcc: str = field_kcc,\n",
    ") -> Union[Tuple[pd.DataFrame, AttributeTable], None]:\n",
    "    \"\"\"\n",
    "    Using climate counts and classification crosswalk, return table\n",
    "        of climates + attribute table of kcc\n",
    "        \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - fp_climate_counts: file path to climate count CSV\n",
    "    - fp_kcc_attribute: file path to kopen climate classifcation attribute\n",
    "        (must contain field_key_kcc)\n",
    "        \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - field_codenum_kcc: field in kcc attribute with code number \n",
    "    - field_iso_climate: field storing iso code\n",
    "    - field_kcc: field storing Kopen Climate Classification code\n",
    "    \"\"\"\n",
    "    \n",
    "    df_climate = (\n",
    "        pd.read_csv(fp_climate_counts)\n",
    "        if os.path.exists(fp_climate_counts)\n",
    "        else None\n",
    "    )\n",
    "    \n",
    "    df_kcc = (\n",
    "        pd.read_csv(fp_kcc_attribute, sep = \",\")\n",
    "        if os.path.exists(fp_kcc_attribute)\n",
    "        else None\n",
    "    )\n",
    "    \n",
    "    # actions to climate counts\n",
    "    if df_climate is not None:\n",
    "        if field_kcc in df_climate.columns:\n",
    "            df_climate[field_kcc] = np.array(df_climate[field_kcc]).astype(int)\n",
    "            \n",
    "        if field_iso_climate in df_climate.columns:\n",
    "            (\n",
    "                df_climate\n",
    "                .rename(\n",
    "                    columns = {field_iso_climate: regions.field_iso}, \n",
    "                    inplace = True\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # actions to attribute\n",
    "    if df_kcc is not None:\n",
    "        \n",
    "        if field_codenum_kcc in df_kcc.columns:\n",
    "            (\n",
    "                df_kcc\n",
    "                .rename(\n",
    "                    columns = {field_codenum_kcc: field_kcc}, \n",
    "                    inplace = True\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # add specification of \n",
    "        #field_specification\n",
    "        \n",
    "        df_kcc = AttributeTable(\n",
    "            df_kcc,\n",
    "            field_kcc,\n",
    "        )\n",
    "        \n",
    "    \n",
    "    if (df_climate is not None) & (df_kcc is not None):\n",
    "        df_climate = (\n",
    "            pd.merge(\n",
    "                df_climate,\n",
    "                df_kcc.table,\n",
    "                how = \"left\"\n",
    "            )\n",
    "            if (field_kcc in df_kcc.table.columns) & (field_kcc in df_climate.columns)\n",
    "            else df_climate\n",
    "        )\n",
    "        \n",
    "    \n",
    "    return df_climate, df_kcc\n",
    "\n",
    "\n",
    "\n",
    "def get_iso_land_use_climate_data(\n",
    "    dir_read: str,\n",
    "    regex_match: re.Pattern,\n",
    "    attr_kcc: AttributeTable,\n",
    "    attr_luc: AttributeTable,\n",
    "    model_attributes: ma.ModelAttributes,\n",
    "    field_kcc: str = field_kcc,\n",
    "    field_luc: str = field_luc,\n",
    "    field_types: Union[Dict[str, str], None] = None,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Build an complete data from of KCC and land use classification (LUC) by\n",
    "        ISO code + attributes \n",
    "    \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - dir_read: directory containing the files to concatenate\n",
    "    - regex_match: regex for files containing the kcc/luc indices by ISO code\n",
    "    - attr_kcc: attribute table for Kopen Climate Classification\n",
    "    - attr_luc: attribute table for land use classes\n",
    "    - model_attributes: model attributes to use for assigning ISO field\n",
    "    \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - field_kcc: output field for Kopen Climate Classification (in compiled \n",
    "        DataFrame built by build_kcc_luc_agg_file())\n",
    "    - field_luc: output field for land use classification (in compiled DataFrame \n",
    "        built by build_kcc_luc_agg_file())\n",
    "    - field_types: optional map of string to Pandas data type to apply for fields\n",
    "    \"\"\"\n",
    "    \n",
    "    # read data from storage and build\n",
    "    df_kcc_and_luc_by_iso = build_kcc_luc_agg_file(\n",
    "        dir_read, \n",
    "        regex_match,\n",
    "        model_attributes,\n",
    "        field_types = field_types,\n",
    "    )\n",
    "    \n",
    "    df_kcc_and_luc_by_iso.sort_values(\n",
    "        by = [\n",
    "            regions.field_iso,\n",
    "            field_count\n",
    "        ],\n",
    "        inplace = True\n",
    "    )\n",
    "    \n",
    "    # merge in attributes\n",
    "    df_kcc_and_luc_by_iso = pd.merge(\n",
    "        df_kcc_and_luc_by_iso,\n",
    "        attr_kcc.table.rename(columns = {attr_kcc.key: field_kcc}),\n",
    "        how = \"left\"\n",
    "    )\n",
    "\n",
    "    df_kcc_and_luc_by_iso = pd.merge(\n",
    "        df_kcc_and_luc_by_iso,\n",
    "        attr_luc.table.rename(columns = {attr_luc.key: field_luc}),\n",
    "        how = \"left\"\n",
    "    )\n",
    "\n",
    "    return df_kcc_and_luc_by_iso\n",
    "\n",
    "\n",
    "\n",
    "def get_luc_name_to_sisepuede_cats(\n",
    "    attr_luc: AttributeTable,\n",
    "    delim: str = \";\",\n",
    "    field_categories_sisepuede: str = field_cats_sisepuede,\n",
    "    field_luc_attr_name: str = field_luc_attr_name,\n",
    ") -> Dict:\n",
    "    \"\"\"\n",
    "    Build dictionary mapping land use category to list of sisepuede land use\n",
    "        categories & inverse\n",
    "    \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - attr_luc: land use category attribute table\n",
    "    \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - delim: delimiter in entries in field_categories_sisepuede\n",
    "    - field_categories_sisepuede: field in attr_luc.table containing sisepuede\n",
    "        categories\n",
    "    - field_luc_attr_name: field in attr_luc.table containing land use category\n",
    "        name\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_luc_to_sc = {}\n",
    "    dict_sc_to_luc = {}\n",
    "    \n",
    "    for i, row in attr_luc.table.iterrows():\n",
    "        \n",
    "        luc = str(row[field_luc_attr_name])\n",
    "        sisepuede_cats = str(row[field_categories_sisepuede]).split(delim)\n",
    "        \n",
    "        dict_luc_to_sc.update({luc: sisepuede_cats})\n",
    "        \n",
    "        for cat in sisepuede_cats: \n",
    "            (\n",
    "                dict_sc_to_luc.update({cat: [luc]})\n",
    "                if cat not in dict_sc_to_luc.keys()\n",
    "                else dict_sc_to_luc[cat].append(luc)\n",
    "            )\n",
    "        \n",
    "    return dict_luc_to_sc, dict_sc_to_luc\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "# read tables\n",
    "df_climate, attr_kcc = get_climate_data(\n",
    "    sa.fp_csv_kcc_cell_counts_by_country_kcc,\n",
    "    fp_kcc_cw\n",
    ")\n",
    "\n",
    "\n",
    "# attribute for land use classification\n",
    "attr_luc = AttributeTable(\n",
    "    fp_attr_luc,\n",
    "    \"land_use_category_index\"\n",
    ")\n",
    "\n",
    "# get land use by climate\n",
    "df_lndu_climate_counts = get_iso_land_use_climate_data(\n",
    "    os.path.join(dir_data, \"KCC_LUC_aggs_by_country\"),\n",
    "    re.compile(\"kcc_and_lndusecat_by_iso_(\\D*)_agg.csv\"),\n",
    "    attr_kcc, \n",
    "    attr_luc,\n",
    "    sa.model_attributes,\n",
    "    field_kcc = field_kcc,\n",
    "    field_luc = field_luc,\n",
    "    field_types = {\n",
    "        field_count: \"int\",\n",
    "        field_kcc: \"int\",\n",
    "        field_luc: \"int\",\n",
    "    }\n",
    ")\n",
    "\n",
    "# get some forest carbon factors\n",
    "df_carbon_factors_forest = get_carbon_factors_forest(fp_ipcc_forest)\n",
    "df_carbon_factors_grassland = get_carbon_factors_grassland(fp_ipcc_grassland)\n",
    "\n",
    "# get some dictionaries\n",
    "dict_luc_to_cats_lndu, dict_cat_lndu_to_lucs = get_luc_name_to_sisepuede_cats(attr_luc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 907,
   "id": "0ad9fbed-310c-4e93-86be-65bd52919228",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>biomass_storage_forest_natural_dm_tonnes_per_ha</th>\n",
       "      <th>count</th>\n",
       "      <th>kcc</th>\n",
       "      <th>Primary</th>\n",
       "      <th>Secondary\\n&gt;20 years</th>\n",
       "      <th>Secondary\\n≤20 years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25.6</td>\n",
       "      <td>22</td>\n",
       "      <td>211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>42.9</td>\n",
       "      <td>58891</td>\n",
       "      <td>221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>44.0</td>\n",
       "      <td>22</td>\n",
       "      <td>211</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>55.7</td>\n",
       "      <td>393</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mangroves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55.7</td>\n",
       "      <td>1660992</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>71.5</td>\n",
       "      <td>1772</td>\n",
       "      <td>120</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Mangroves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>71.5</td>\n",
       "      <td>2490921</td>\n",
       "      <td>120</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>74.6</td>\n",
       "      <td>25999</td>\n",
       "      <td>322</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>75.7</td>\n",
       "      <td>465</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mangroves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75.7</td>\n",
       "      <td>1647292</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>84.5</td>\n",
       "      <td>3</td>\n",
       "      <td>331</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Mangroves</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>84.5</td>\n",
       "      <td>73167</td>\n",
       "      <td>321</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>84.5</td>\n",
       "      <td>219157</td>\n",
       "      <td>331</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>115.9</td>\n",
       "      <td>237</td>\n",
       "      <td>311</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>115.9</td>\n",
       "      <td>355</td>\n",
       "      <td>312</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>118.5</td>\n",
       "      <td>58891</td>\n",
       "      <td>221</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>131.0</td>\n",
       "      <td>393</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>131.0</td>\n",
       "      <td>1660992</td>\n",
       "      <td>130</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>187.3</td>\n",
       "      <td>393</td>\n",
       "      <td>130</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>187.3</td>\n",
       "      <td>1660992</td>\n",
       "      <td>130</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>206.4</td>\n",
       "      <td>465</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>206.4</td>\n",
       "      <td>1647292</td>\n",
       "      <td>110</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>213.9</td>\n",
       "      <td>164448</td>\n",
       "      <td>332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>307.1</td>\n",
       "      <td>465</td>\n",
       "      <td>110</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>307.1</td>\n",
       "      <td>1647292</td>\n",
       "      <td>110</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>354.1</td>\n",
       "      <td>164448</td>\n",
       "      <td>332</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    biomass_storage_forest_natural_dm_tonnes_per_ha    count  kcc  \\\n",
       "0                                              25.6       22  211   \n",
       "1                                              42.9    58891  221   \n",
       "2                                              44.0       22  211   \n",
       "3                                              55.7      393  130   \n",
       "4                                              55.7  1660992  130   \n",
       "5                                              71.5     1772  120   \n",
       "6                                              71.5  2490921  120   \n",
       "7                                              74.6    25999  322   \n",
       "8                                              75.7      465  110   \n",
       "9                                              75.7  1647292  110   \n",
       "10                                             84.5        3  331   \n",
       "11                                             84.5    73167  321   \n",
       "12                                             84.5   219157  331   \n",
       "13                                            115.9      237  311   \n",
       "14                                            115.9      355  312   \n",
       "15                                            118.5    58891  221   \n",
       "16                                            131.0      393  130   \n",
       "17                                            131.0  1660992  130   \n",
       "18                                            187.3      393  130   \n",
       "19                                            187.3  1660992  130   \n",
       "20                                            206.4      465  110   \n",
       "21                                            206.4  1647292  110   \n",
       "22                                            213.9   164448  332   \n",
       "23                                            307.1      465  110   \n",
       "24                                            307.1  1647292  110   \n",
       "25                                            354.1   164448  332   \n",
       "\n",
       "               Primary Secondary\\n>20 years Secondary\\n≤20 years  \n",
       "0                  NaN                  NaN   Tree Covered Areas  \n",
       "1                  NaN                  NaN   Tree Covered Areas  \n",
       "2                  NaN   Tree Covered Areas                  NaN  \n",
       "3                  NaN                  NaN            Mangroves  \n",
       "4                  NaN                  NaN   Tree Covered Areas  \n",
       "5            Mangroves            Mangroves            Mangroves  \n",
       "6   Tree Covered Areas   Tree Covered Areas   Tree Covered Areas  \n",
       "7   Tree Covered Areas   Tree Covered Areas   Tree Covered Areas  \n",
       "8                  NaN                  NaN            Mangroves  \n",
       "9                  NaN                  NaN   Tree Covered Areas  \n",
       "10           Mangroves            Mangroves            Mangroves  \n",
       "11  Tree Covered Areas   Tree Covered Areas   Tree Covered Areas  \n",
       "12  Tree Covered Areas   Tree Covered Areas   Tree Covered Areas  \n",
       "13  Tree Covered Areas   Tree Covered Areas   Tree Covered Areas  \n",
       "14  Tree Covered Areas   Tree Covered Areas   Tree Covered Areas  \n",
       "15                 NaN   Tree Covered Areas                  NaN  \n",
       "16                 NaN            Mangroves                  NaN  \n",
       "17                 NaN   Tree Covered Areas                  NaN  \n",
       "18           Mangroves                  NaN                  NaN  \n",
       "19  Tree Covered Areas                  NaN                  NaN  \n",
       "20                 NaN            Mangroves                  NaN  \n",
       "21                 NaN   Tree Covered Areas                  NaN  \n",
       "22                 NaN                  NaN   Tree Covered Areas  \n",
       "23           Mangroves                  NaN                  NaN  \n",
       "24  Tree Covered Areas                  NaN                  NaN  \n",
       "25                 NaN   Tree Covered Areas                  NaN  "
      ]
     },
     "execution_count": 907,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_means_forest_storage_0 = clean_dfm(\n",
    "    df_means_forest_storage_0,\n",
    "    field_storage_natural,\n",
    "    field_count,\n",
    "    field_kcc,\n",
    "    field_luc_attr_name,\n",
    "    field_type_forest,\n",
    ")\n",
    "\n",
    "sf.pivot_df_clean(\n",
    "        df_means_forest_storage_0,\n",
    "        [field_type_forest],\n",
    "        [field_growth_natural, field_luc_attr_name]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "id": "920c0fae-99db-4803-9fb0-13da0b08fcf0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>kcc</th>\n",
       "      <th>land_use_category_name</th>\n",
       "      <th>forest_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>465</td>\n",
       "      <td>110</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>465</td>\n",
       "      <td>110</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>465</td>\n",
       "      <td>110</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1647292</td>\n",
       "      <td>110</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1647292</td>\n",
       "      <td>110</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1647292</td>\n",
       "      <td>110</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1772</td>\n",
       "      <td>120</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1772</td>\n",
       "      <td>120</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1772</td>\n",
       "      <td>120</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2490921</td>\n",
       "      <td>120</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2490921</td>\n",
       "      <td>120</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2490921</td>\n",
       "      <td>120</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>393</td>\n",
       "      <td>130</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>393</td>\n",
       "      <td>130</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>393</td>\n",
       "      <td>130</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1660992</td>\n",
       "      <td>130</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1660992</td>\n",
       "      <td>130</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1660992</td>\n",
       "      <td>130</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>22</td>\n",
       "      <td>211</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>22</td>\n",
       "      <td>211</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>58891</td>\n",
       "      <td>221</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>58891</td>\n",
       "      <td>221</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>237</td>\n",
       "      <td>311</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>237</td>\n",
       "      <td>311</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>237</td>\n",
       "      <td>311</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>355</td>\n",
       "      <td>312</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Primary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>355</td>\n",
       "      <td>312</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>355</td>\n",
       "      <td>312</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>73167</td>\n",
       "      <td>321</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>73167</td>\n",
       "      <td>321</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>25999</td>\n",
       "      <td>322</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>25999</td>\n",
       "      <td>322</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>3</td>\n",
       "      <td>331</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>3</td>\n",
       "      <td>331</td>\n",
       "      <td>Mangroves</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>219157</td>\n",
       "      <td>331</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>219157</td>\n",
       "      <td>331</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>164448</td>\n",
       "      <td>332</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n&gt;20 years</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>164448</td>\n",
       "      <td>332</td>\n",
       "      <td>Tree Covered Areas</td>\n",
       "      <td>Secondary\\n≤20 years</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      count  kcc land_use_category_name           forest_type\n",
       "0       465  110              Mangroves               Primary\n",
       "1       465  110              Mangroves  Secondary\\n>20 years\n",
       "2       465  110              Mangroves  Secondary\\n≤20 years\n",
       "3   1647292  110     Tree Covered Areas               Primary\n",
       "4   1647292  110     Tree Covered Areas  Secondary\\n>20 years\n",
       "5   1647292  110     Tree Covered Areas  Secondary\\n≤20 years\n",
       "6      1772  120              Mangroves               Primary\n",
       "7      1772  120              Mangroves  Secondary\\n>20 years\n",
       "8      1772  120              Mangroves  Secondary\\n≤20 years\n",
       "9   2490921  120     Tree Covered Areas               Primary\n",
       "10  2490921  120     Tree Covered Areas  Secondary\\n>20 years\n",
       "11  2490921  120     Tree Covered Areas  Secondary\\n≤20 years\n",
       "12      393  130              Mangroves               Primary\n",
       "13      393  130              Mangroves  Secondary\\n>20 years\n",
       "14      393  130              Mangroves  Secondary\\n≤20 years\n",
       "15  1660992  130     Tree Covered Areas               Primary\n",
       "16  1660992  130     Tree Covered Areas  Secondary\\n>20 years\n",
       "17  1660992  130     Tree Covered Areas  Secondary\\n≤20 years\n",
       "18       22  211     Tree Covered Areas  Secondary\\n>20 years\n",
       "19       22  211     Tree Covered Areas  Secondary\\n≤20 years\n",
       "20    58891  221     Tree Covered Areas  Secondary\\n>20 years\n",
       "21    58891  221     Tree Covered Areas  Secondary\\n≤20 years\n",
       "22      237  311     Tree Covered Areas               Primary\n",
       "23      237  311     Tree Covered Areas  Secondary\\n>20 years\n",
       "24      237  311     Tree Covered Areas  Secondary\\n≤20 years\n",
       "25      355  312     Tree Covered Areas               Primary\n",
       "26      355  312     Tree Covered Areas  Secondary\\n>20 years\n",
       "27      355  312     Tree Covered Areas  Secondary\\n≤20 years\n",
       "28    73167  321     Tree Covered Areas  Secondary\\n>20 years\n",
       "29    73167  321     Tree Covered Areas  Secondary\\n≤20 years\n",
       "30    25999  322     Tree Covered Areas  Secondary\\n>20 years\n",
       "31    25999  322     Tree Covered Areas  Secondary\\n≤20 years\n",
       "32        3  331              Mangroves  Secondary\\n>20 years\n",
       "33        3  331              Mangroves  Secondary\\n≤20 years\n",
       "34   219157  331     Tree Covered Areas  Secondary\\n>20 years\n",
       "35   219157  331     Tree Covered Areas  Secondary\\n≤20 years\n",
       "36   164448  332     Tree Covered Areas  Secondary\\n>20 years\n",
       "37   164448  332     Tree Covered Areas  Secondary\\n≤20 years"
      ]
     },
     "execution_count": 926,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = clean_dfm(\n",
    "        df_means_forest_growth_0,\n",
    "        field_storage_natural,\n",
    "        field_count,\n",
    "        field_kcc,\n",
    "        field_luc_attr_name,\n",
    "        field_type_forest,\n",
    "    )\n",
    "\n",
    "sf.pivot_df_clean(\n",
    "    df,\n",
    "    [field_type_forest, field_luc_attr_name],\n",
    "    [field_growth_natural]\n",
    ")\n",
    "df_means_forest_growth_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "id": "678f2cc3-7eab-4aff-bf7c-4649e1f14c26",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ISO ABW failed\n",
      "ISO AND failed\n",
      "ISO ARM failed\n",
      "ISO ASM failed\n",
      "ISO ATG failed\n",
      "ISO BLZ failed\n",
      "ISO BMU failed\n",
      "ISO BRB failed\n",
      "ISO BRN failed\n",
      "ISO COM failed\n",
      "ISO CPV failed\n",
      "ISO CYM failed\n",
      "ISO DJI failed\n",
      "ISO DMA failed\n",
      "ISO FJI failed\n",
      "ISO FRO failed\n",
      "ISO FSM failed\n",
      "ISO GAB failed\n",
      "ISO GIB failed\n",
      "ISO GNQ failed\n",
      "ISO GRD failed\n",
      "ISO GRL failed\n",
      "ISO GUM failed\n",
      "ISO GUY failed\n",
      "ISO HKG failed\n",
      "ISO HTI failed\n",
      "ISO IMN failed\n",
      "ISO KIR failed\n",
      "ISO KNA failed\n",
      "ISO KWT failed\n",
      "ISO LBR failed\n",
      "ISO LCA failed\n",
      "ISO LIE failed\n",
      "ISO LSO failed\n",
      "ISO LUX failed\n",
      "ISO MAC failed\n",
      "ISO MAF failed\n",
      "ISO MCO failed\n",
      "ISO MDG failed\n",
      "ISO MDV failed\n",
      "ISO MHL failed\n",
      "ISO MLT failed\n",
      "ISO MNP failed\n",
      "ISO MUS failed\n",
      "ISO MYS failed\n",
      "ISO NCL failed\n",
      "ISO NRU failed\n",
      "ISO PHL failed\n",
      "ISO PLW failed\n",
      "ISO PYF failed\n",
      "ISO SGP failed\n",
      "ISO SLB failed\n",
      "ISO SLV failed\n",
      "ISO SMR failed\n",
      "ISO STP failed\n",
      "ISO SUR failed\n",
      "ISO SXM failed\n",
      "ISO SYC failed\n",
      "ISO TCA failed\n",
      "ISO TJK failed\n",
      "ISO TLS failed\n",
      "ISO TON failed\n",
      "ISO TTO failed\n",
      "ISO TUV failed\n",
      "ISO VCT failed\n",
      "ISO VGB failed\n",
      "ISO VUT failed\n",
      "ISO WSM failed\n"
     ]
    }
   ],
   "source": [
    "lucs_with_no_biomass = [\n",
    "    \"Artificial Surfaces\",\n",
    "    \"Baresoil\", # biomass is estimated directly in cropland, so assume all carbon is lost\n",
    "    \"Cropland\",\n",
    "    \"Snow and glaciers\",\n",
    "    \"Waterbodies\",\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "# map land use categories (applicable) to forest types to use from ipcc_forest\n",
    "dict_cats_lndu_to_type_forest = {\n",
    "    \"forests_mangroves\": [\n",
    "        \"Primary\",\n",
    "        \"Secondary\\n>20 years\",\n",
    "        \"Secondary\\n≤20 years\"\n",
    "    ],\n",
    "    \"forests_primary\": [\n",
    "        \"Primary\"\n",
    "    ],\n",
    "    \"forests_secondary\": [\n",
    "        \"Secondary\\n>20 years\",\n",
    "        \"Secondary\\n≤20 years\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "def clean_dfm(\n",
    "    dfm: pd.DataFrame,\n",
    "    field_agg: str,\n",
    "    field_count: str,\n",
    "    field_kcc: str,\n",
    "    field_luc_attr_name: str,\n",
    "    field_type_forest: str,\n",
    ") -> pd.DataFrame:\n",
    "    \n",
    "    dfg = dfm.groupby([field_kcc, field_luc_attr_name, field_type_forest])\n",
    "    \n",
    "    df_out = []\n",
    "    for tup, df in dfg:\n",
    "        df = sf.simple_df_agg(\n",
    "            df.dropna(),\n",
    "            [field_kcc, field_luc_attr_name, field_type_forest],\n",
    "            {\n",
    "                field_agg: \"max\",\n",
    "                field_count: \"sum\",\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        df_out.append(df)\n",
    "    \n",
    "    df_out = pd.concat(df_out, axis = 0).reset_index(drop = True)\n",
    "    \n",
    "    return df_out\n",
    "\n",
    "\n",
    "def get_factor_info(\n",
    "    df_means: pd.DataFrame,\n",
    "    dict_cat_lndu_to_lucs: Dict[str, List[str]],\n",
    "    dict_luc_to_cats_lndu: Dict[str, List[str]],\n",
    "    iso: str, \n",
    "    default_c_per_biomass: float = 0.47,\n",
    "    field_count: str = field_count,\n",
    "    field_growth_forest: str = field_growth_natural,\n",
    "    field_growth_forest_plantation: str = field_growth_plantation,\n",
    "    field_iso: str = regions.field_iso,\n",
    "    field_luc_name: str = field_luc_attr_name,\n",
    "    field_storage_forest: str = field_storage_natural,\n",
    "    field_storage_forest_plantation: str = field_storage_plantation,\n",
    "    field_storage_grassland: str = field_storage_grassland,\n",
    "    field_storage_grassland_total: str = field_storage_grassland_total,\n",
    "    field_tmp: str = \"tmp\",\n",
    "    field_type_factor: str = field_type_factor,\n",
    "    field_type_forest: str = field_type_forest,\n",
    "    field_type_forest_ipcc: str = field_type_forest_ipcc,\n",
    "    lucs_zero: List[str] = lucs_with_no_biomass,\n",
    "    model_afolu: mafl.AFOLU = model_afolu,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Map df_means in get_average_forest_factors_by_iso() to biomass estimates\n",
    "        for conversion between Land Use Categories\n",
    "        \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - df_means: input df_means data frame\n",
    "    - iso: iso code to attach\n",
    "    \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - default_c_per_dm: default c/biomass (see IPCC V4 Table 4.3 2006)\n",
    "    - lucs_zero: list of categories with \n",
    "    \"\"\"\n",
    "    # some key init\n",
    "    attr_frst = model_afolu.model_attributes.get_attribute_table(\n",
    "        model_afolu.model_attributes.subsec_name_frst\n",
    "    )\n",
    "    attr_lndu = model_afolu.model_attributes.get_attribute_table(\n",
    "        model_afolu.model_attributes.subsec_name_lndu\n",
    "    )\n",
    "    count_total = int(df_means[field_count].sum())\n",
    "    cats_forest = list(model_afolu.dict_cats_frst_to_cats_lndu.values())\n",
    "    time_periods = sc.TimePeriods(model_afolu.model_attributes)\n",
    "    \n",
    "    ##  SET SOME LAND USE CATEGORY GROUPS\n",
    "     \n",
    "    lucs_zero = (\n",
    "        lucs_zero\n",
    "        if sf.islistlike(lucs_zero)\n",
    "        else []\n",
    "    )\n",
    "    \n",
    "    # forest types\n",
    "    lucs_forest = set(\n",
    "        sum(\n",
    "            [\n",
    "                dict_cat_lndu_to_lucs.get(x)\n",
    "                for x in list(model_afolu.dict_cats_frst_to_cats_lndu.values())\n",
    "            ], \n",
    "            []\n",
    "        )\n",
    "    )\n",
    "    #HEREHERE\n",
    "    global df_means_forest_growth_0\n",
    "    global df_means_forest_storage_0\n",
    "    # split means into forest/not-forest (grassland)\n",
    "    df_means_forest = (\n",
    "        df_means[\n",
    "            [(x in lucs_forest) for x in df_means[field_luc_attr_name]]\n",
    "        ]\n",
    "        .drop([field_storage_grassland, field_storage_grassland_total], axis = 1)\n",
    "        .reset_index(drop = True)\n",
    "    )\n",
    "    fields_ind = [field_kcc, field_count, field_luc_attr_name, field_type_forest]\n",
    "    \n",
    "    df_means_forest_growth = df_means_forest[fields_ind + [field_growth_forest]]\n",
    "    df_means_forest_growth = clean_dfm(\n",
    "        df_means_forest_growth,\n",
    "        field_growth_forest,\n",
    "        field_count,\n",
    "        field_kcc,\n",
    "        field_luc_attr_name,\n",
    "        field_type_forest,\n",
    "    )\n",
    "    \n",
    "    df_means_forest_storage = df_means_forest[fields_ind + [field_storage_forest]]\n",
    "    df_means_forest_storage = clean_dfm(\n",
    "        df_means_forest_storage,\n",
    "        field_storage_forest,\n",
    "        field_count,\n",
    "        field_kcc,\n",
    "        field_luc_attr_name,\n",
    "        field_type_forest,\n",
    "    )\n",
    "    \n",
    "    df_means_forest_growth_0 = df_means_forest_growth.copy()\n",
    "    df_means_forest_storage_0 = df_means_forest_storage.copy()\n",
    "    \n",
    "    # pivot \n",
    "    df_means_forest_growth = sf.pivot_df_clean(\n",
    "        df_means_forest_growth,\n",
    "        [field_type_forest, field_luc_attr_name],\n",
    "        [field_growth_forest]\n",
    "    )\n",
    "    df_means_forest_storage = sf.pivot_df_clean(\n",
    "        df_means_forest_storage,\n",
    "        [field_type_forest, field_luc_attr_name],\n",
    "        [field_storage_forest]\n",
    "    )\n",
    "    \n",
    "    \n",
    "   \n",
    "    \n",
    "    \n",
    "    # get non-forest values\n",
    "    df_means_not_forest = (\n",
    "        df_means[\n",
    "            [(x not in lucs_forest) for x in df_means[field_luc_attr_name]]\n",
    "        ]\n",
    "        .drop(\n",
    "            [\n",
    "                field_growth_forest,\n",
    "                field_growth_forest_plantation,\n",
    "                field_storage_forest,\n",
    "                field_storage_forest_plantation,\n",
    "                field_type_forest,\n",
    "                field_type_forest_ipcc,\n",
    "                field_storage_grassland_total,\n",
    "            ],\n",
    "            axis = 1\n",
    "        )\n",
    "        .drop_duplicates()\n",
    "        .reset_index(drop = True)\n",
    "    )\n",
    "    \n",
    "    \n",
    "    ##  GET FOREST FACTORS\n",
    "    \n",
    "    df_means_forest_storage_grouped = df_means_forest_storage.groupby([field_kcc])\n",
    "\n",
    "    for cat in cats_forest:\n",
    "        \n",
    "        fields_retrieve = dict_cats_lndu_to_type_forest.get(cat)\n",
    "        \n",
    "        if set(fields_retrieve).issubset(set(df_means_forest_growth.columns)):\n",
    "            # update growth\n",
    "            df_means_forest_growth[cat] = df_means_forest_growth[fields_retrieve].mean(axis = 1)\n",
    "            df_means_forest_growth[f\"max_{cat}\"] = df_means_forest_growth[fields_retrieve].max(axis = 1)\n",
    "            df_means_forest_growth[f\"min_{cat}\"] = df_means_forest_growth[fields_retrieve].min(axis = 1)\n",
    "            \n",
    "        else:\n",
    "            df_means_forest_growth[cat] = 0\n",
    "            df_means_forest_growth[f\"max_{cat}\"] = 0\n",
    "            df_means_forest_growth[f\"min_{cat}\"] = 0\n",
    "            \n",
    "            \n",
    "        if set(fields_retrieve).issubset(set(df_means_forest_storage.columns)):\n",
    "            #update storage\n",
    "            vec_mean_storage = df_means_forest_storage[fields_retrieve].mean(axis = 1)\n",
    "            df_means_forest_storage[cat] = vec_mean_storage\n",
    "        else:\n",
    "            df_means_forest_storage[cat] = 0.0\n",
    "        \n",
    "       \n",
    "    \n",
    "    # build growth\n",
    "    arr_growth = np.zeros((3, len(cats_forest)))\n",
    "    vec_type = [\"nominal\", \"max\", \"min\"]\n",
    "    for i, fldp in enumerate([\"\", \"max_\", \"min_\"]):\n",
    "        \n",
    "        fields = [f\"{fldp}{x}\" for x in cats_forest]\n",
    "        # get mean sequestration factors\n",
    "        vec_count_forest = np.array(df_means_forest_growth[field_count]).astype(float)\n",
    "        vec_growth = sf.do_array_mult(\n",
    "            np.array(df_means_forest_growth[fields]),\n",
    "            vec_count_forest\n",
    "        )\n",
    "        vec_growth /= vec_count_forest.sum()\n",
    "\n",
    "        # convert 0s, then convert dry matter to CO2e\n",
    "        vec_growth = np.nan_to_num(vec_growth, 0.0).sum(axis = 0)\n",
    "        vec_growth *= default_c_per_biomass\n",
    "        vec_growth *= model_afolu.factor_c_to_co2\n",
    "        \n",
    "        arr_growth[i, :] = vec_growth/1000\n",
    "    \n",
    "    # output growth\n",
    "    df_growth = pd.DataFrame(arr_growth, columns = cats_forest)\n",
    "    df_growth[field_type_factor] = vec_type\n",
    "    df_growth[field_iso] = iso\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##  NEXT, GET STORAGE FOR OTHER LU TYPES\n",
    "    \n",
    "    vec_bin = np.array(\n",
    "        [(x not in lucs_zero) for x in (df_means_not_forest[field_luc_attr_name])]\n",
    "    )\n",
    "    vec_count = np.array(df_means_not_forest[field_count])\n",
    "    df_means_not_forest[field_luc_attr_name].replace(dict_luc_to_cats_lndu, inplace = True)\n",
    "    df_means_not_forest[field_storage_grassland] *= vec_bin\n",
    "\n",
    "    # normalize by climate\n",
    "    df_mnf = []\n",
    "    df_means_not_forest_grouped = (\n",
    "        df_means_not_forest.\n",
    "        dropna()\n",
    "        .groupby([field_kcc, field_luc_attr_name])\n",
    "    )\n",
    "\n",
    "    for kcc, df in df_means_not_forest_grouped:\n",
    "        vec_count = np.array(df[field_count])\n",
    "        vec_count = vec_count/vec_count.sum()\n",
    "        df[field_storage_grassland] *= vec_count\n",
    "        df = sf.simple_df_agg(\n",
    "            df,\n",
    "            [field_kcc, field_luc_attr_name],\n",
    "            {\n",
    "                field_count: \"sum\",\n",
    "                field_storage_grassland: \"sum\"\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        df_mnf.append(df)\n",
    "    df_means_not_forest = pd.concat(df_mnf, axis = 0)\n",
    "    df_counts_means_not_forest = (\n",
    "        sf.simple_df_agg(\n",
    "            df_means_not_forest[[field_count, field_kcc]],\n",
    "            [field_kcc],\n",
    "            {field_count: \"sum\"}\n",
    "        )\n",
    "        .rename(columns = {field_count: f\"nf_{field_count}\"})\n",
    "    )\n",
    "    df_means_not_forest = pd.merge(\n",
    "        sf.pivot_df_clean(\n",
    "            df_means_not_forest.drop([field_count], axis = 1),\n",
    "            [field_luc_attr_name],\n",
    "            [field_storage_grassland]\n",
    "        ),\n",
    "        df_counts_means_not_forest,\n",
    "        how = \"left\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##  GET STORAGE AND CONVERSION\n",
    "    \n",
    "    # initialize a dictionary of weight mean storage\n",
    "    dict_storage_mu = {}\n",
    "    \n",
    "    #  reducting forest data frame\n",
    "    df_counts_means_forest = (\n",
    "        sf.simple_df_agg(\n",
    "            df_means_forest[[field_count, field_kcc]],\n",
    "            [field_kcc],\n",
    "            {field_count: \"sum\"}\n",
    "        )\n",
    "        .rename(columns = {field_count: f\"f_{field_count}\"})\n",
    "    )\n",
    "    df_means_forest_storage = pd.merge(\n",
    "        df_means_forest_storage[[field_kcc] + cats_forest].drop_duplicates(),\n",
    "        df_counts_means_forest,\n",
    "        how = \"left\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    # get weighted means for FOREST\n",
    "    vec_counts = np.array(df_means_forest_storage[f\"f_{field_count}\"])\n",
    "    for field in [x for x in df_means_forest_storage.columns if x in attr_lndu.key_values]:\n",
    "        val = np.array(df_means_forest_storage[field])\n",
    "        w = np.where(~np.isnan(val))[0]\n",
    "        val = (\n",
    "            (val[w]*vec_counts[w]/vec_counts[w].sum()).sum()\n",
    "            if len(w) > 0\n",
    "            else 0\n",
    "        )\n",
    "        \n",
    "        dict_storage_mu.update({field: val})\n",
    "        \n",
    "    # get weights means for NOT FOREST\n",
    "    vec_counts = np.array(df_means_not_forest[f\"nf_{field_count}\"])\n",
    "    for field in [x for x in df_means_not_forest.columns if x in attr_lndu.key_values]:\n",
    "        val = np.array(df_means_not_forest[field])\n",
    "        w = np.where(~np.isnan(val))[0]\n",
    "        val = (\n",
    "            (val[w]*vec_counts[w]/vec_counts[w].sum()).sum()\n",
    "            if len(w) > 0\n",
    "            else 0\n",
    "        )\n",
    "        \n",
    "        dict_storage_mu.update({field: val})\n",
    "    \n",
    "    global dfnf\n",
    "    global dfmfs\n",
    "    dfnf = df_means_not_forest\n",
    "    dfmfs = df_means_forest_storage\n",
    "    \n",
    "    global df_conversions\n",
    "    \n",
    "    # get final data frame used to calculate conversions by climate\n",
    "    df_conversions = (\n",
    "        pd.merge(\n",
    "            df_means_not_forest,\n",
    "            df_means_forest_storage,\n",
    "            how = \"outer\"\n",
    "        )\n",
    "    )\n",
    "    fields_conv = [f\"nf_{field_count}\", f\"f_{field_count}\"]\n",
    "    df_conversions[field_count] = df_conversions[fields_conv].sum(axis = 1)\n",
    "    df_conversions.drop(fields_conv, axis = 1, inplace = True)\n",
    "    df_conversions.fillna(\n",
    "        dict((k, v) for k, v in dict_storage_mu.items() if k not in cats_forest), \n",
    "        inplace = True\n",
    "    )\n",
    "\n",
    "    # iterate over rows to build weighted conversion factors\n",
    "    num = 0.0\n",
    "    denom = 0.0#np.zeros((attr_lndu.n_key_values, attr_lndu.n_key_values))\n",
    "\n",
    "    for i, row in df_conversions.iterrows():\n",
    "    \n",
    "        vec = np.array(row[attr_lndu.key_values])\n",
    "        count = int(row[field_count])\n",
    "        \n",
    "        storage_rows = np.outer(vec, np.ones(attr_lndu.n_key_values))\n",
    "        storage_cols = np.outer(np.ones(attr_lndu.n_key_values), vec)\n",
    "        arr_conv = sf.vec_bounds(storage_rows - storage_cols, (0, np.inf))\n",
    "        \n",
    "        tot = count*np.ones((attr_lndu.n_key_values, attr_lndu.n_key_values))\n",
    "        tot[np.isnan(arr_conv)] = 0\n",
    "        #print(tot*arr_conv)\n",
    "        num += np.nan_to_num(arr_conv, 0.0)*tot\n",
    "        denom += tot\n",
    "    \n",
    "    arr_conv = np.nan_to_num(num/denom, nan = 0, posinf = 0)\n",
    "    arr_conv *= default_c_per_biomass\n",
    "    arr_conv *= model_afolu.factor_c_to_co2\n",
    "    arr_conv /= 1000\n",
    "    arr_conv = np.round(arr_conv, decimals = 6)\n",
    "    \n",
    "    # convert to data frame\n",
    "    df_efs = model_afolu.format_transition_matrix_as_input_dataframe(\n",
    "        np.array([arr_conv for x in time_periods.all_time_periods]),\n",
    "        modvar = model_afolu.modvar_lndu_ef_co2_conv,\n",
    "    )\n",
    "    df_efs[field_iso] = iso\n",
    "    \n",
    "    \n",
    "    ##  CLEAN SEQUESTRATION DF\n",
    "    \n",
    "    # reduce growth out\n",
    "    cats_forest = [x for x in attr_lndu.key_values if x in cats_forest]\n",
    "    cats_forest_frst = [x for x in attr_frst.key_values if model_afolu.dict_cats_frst_to_cats_lndu.get(x) in cats_forest]\n",
    "    \n",
    "    dict_rnm = dict(\n",
    "        zip(\n",
    "            cats_forest,\n",
    "            model_afolu.model_attributes.build_varlist(\n",
    "                None,\n",
    "                model_afolu.modvar_frst_sq_co2,\n",
    "                restrict_to_category_values = cats_forest_frst,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df_growth_out = (\n",
    "        df_growth[\n",
    "            df_growth[field_type_factor].isin([\"nominal\"])\n",
    "        ]\n",
    "        .reset_index(drop = True)\n",
    "        .drop([field_type_factor], axis = 1)\n",
    "        .rename(columns = dict_rnm)\n",
    "    )\n",
    "    df_growth_out = sf.explode_merge(\n",
    "        time_periods.get_time_period_df(),\n",
    "        df_growth_out\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return df_growth_out, df_efs\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_average_forest_factors_by_iso(\n",
    "    df_climate_by_iso: pd.DataFrame,\n",
    "    df_carbon_factors_forest: pd.DataFrame,\n",
    "    df_carbon_factors_grassland: pd.DataFrame,\n",
    "    dict_luc_to_cats_lndu: Dict[str, List[str]],\n",
    "    attr_kcc: AttributeTable,\n",
    "    regions: sc.Regions,\n",
    "    field_continent: str = field_continent,\n",
    "    field_count: str = \"count\",\n",
    "    field_ecological_zone: str = \"ecological_zone1\",\n",
    "    field_forest_cat: str = \"ipcc_forest\",\n",
    "    field_type_forest: str = field_type_forest,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate average storage and sequestration rate factors by iso code\n",
    "    \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - df_climate_by_iso: data frame containing KCC climate counts by ISO\n",
    "        code\n",
    "    - df_carbon_factors_forest: data frame storing IPCC GHG default carbon \n",
    "        biomass factors for forest (IPCC table V4.4.12)\n",
    "    - df_carbon_factors_grassland: data frame storing IPCC GHG default carbon \n",
    "        biomass factors for forest (IPCC table V4.6.4)\n",
    "    - dict_luc_to_cats_lndu: dictionary mapping land use category to SISEPUEDE\n",
    "        LNDU categories\n",
    "    - attr_kcc: attribute table characterizing Kopen Climate \n",
    "        Classification \n",
    "        \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - field_continent: field storing continent\n",
    "    - field_count: field giving # of cells by country assocaited with \n",
    "        each KCC category\n",
    "    - field_ecological_zone: field in df_carbon_factors_forest containing IPCC\n",
    "        forests\n",
    "    - field_forest_cat: field in df_climate_by_iso containing the IPCC \n",
    "        forest category used to estimate factors\n",
    "    - field_type_forest: field in df_carbon_factors_forest storing forest type\n",
    "    \"\"\"\n",
    "    \n",
    "    dict_un_region_to_continent = {\n",
    "        \"Americas\": \"North and South America\"\n",
    "    }\n",
    "    continents_global = [\n",
    "        \"Asia\\nEurope\\nNorth and South America\",\n",
    "        \"Asia Europe North\\nAmerica\",\n",
    "        \"Asia\\nEurope\\nNorth\\nAmerica\"\n",
    "    ]\n",
    "    \n",
    "    fields_keep = [\n",
    "        regions.field_iso,\n",
    "        attr_kcc.key,\n",
    "        field_count,\n",
    "        field_luc_attr_name,\n",
    "        field_forest_cat\n",
    "    ]\n",
    "    dfg_climate = (#TEMP [df_climate_by_iso[regions.field_iso].isin([\"BRA\"])]\n",
    "        df_climate_by_iso[fields_keep]\n",
    "        .groupby([regions.field_iso])\n",
    "    )\n",
    "    df_out_conv = []\n",
    "    df_out_sequestration = []\n",
    "\n",
    "    # clean carbon factors df\n",
    "    df_cf = (\n",
    "        df_carbon_factors_forest\n",
    "        .rename(\n",
    "            columns = {\n",
    "                field_ecological_zone: field_forest_cat\n",
    "            }\n",
    "        )\n",
    "    )\n",
    "    fields_ind_cf = [field_forest_cat, field_continent, field_type_forest]\n",
    "    fields_dat_cf = [x for x in df_cf.columns if x not in fields_ind_cf]\n",
    "\n",
    "    # split into continent-specific and global\n",
    "    df_cf_by_continent = (\n",
    "        df_cf[\n",
    "            ~df_cf[field_continent].isin(continents_global)\n",
    "        ]\n",
    "        .reset_index(drop = True)\n",
    "    )\n",
    "    df_cf_global = (\n",
    "        df_cf[\n",
    "            df_cf[field_continent].isin(continents_global)\n",
    "        ]\n",
    "        .reset_index(drop = True)\n",
    "    )\n",
    "    \n",
    "    # get forest vals by split\n",
    "    all_forests_by_continent = set(df_cf_by_continent[field_forest_cat])\n",
    "    all_forests_global = set(df_cf_global[field_forest_cat])\n",
    "    \n",
    "  \n",
    "    for iso, df in dfg_climate:\n",
    "        \n",
    "        # get un region\n",
    "        region_un = regions.get_un_region(iso)\n",
    "        region_ipcc_forests = dict_un_region_to_continent.get(region_un, region_un)\n",
    "        \n",
    "        all_forests_cur = set(df[field_forest_cat])\n",
    "        any_by_continent = len(all_forests_cur & all_forests_by_continent) > 0\n",
    "        any_global = len(all_forests_cur & all_forests_global) > 0\n",
    "        \n",
    "        # total number of cells\n",
    "        total_count = df[field_count].sum()\n",
    "        \n",
    "        # initialize splits\n",
    "        df_by_continent = None\n",
    "        df_global = None\n",
    "        \n",
    "        # deal with splits\n",
    "        if any_by_continent:\n",
    "            df[field_continent] = region_ipcc_forests\n",
    "\n",
    "            df_by_continent = (\n",
    "                pd.merge(\n",
    "                    df,\n",
    "                    df_cf_by_continent,\n",
    "                )\n",
    "                .drop(\n",
    "                    [\n",
    "                        field_continent, \n",
    "                        #field_kcc, \n",
    "                        #field_forest_cat,\n",
    "                        regions.field_iso\n",
    "                    ], \n",
    "                    axis = 1\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            df.drop([field_continent], axis = 1, inplace = True)\n",
    "            \n",
    "            \n",
    "        if any_global:\n",
    "            df_global = (\n",
    "                pd.merge(\n",
    "                    df,\n",
    "                    df_cf_global.drop([field_continent], axis = 1),\n",
    "                )\n",
    "                .drop(\n",
    "                    [\n",
    "                        #field_kcc, \n",
    "                        #field_forest_cat,\n",
    "                        regions.field_iso\n",
    "                    ], \n",
    "                    axis = 1\n",
    "                )\n",
    "            )\n",
    "\n",
    "        \n",
    "        # check that at least one was successfully merges\n",
    "        if (df_global is None) & (df_by_continent is None):\n",
    "            continue\n",
    "        \n",
    "        global df_means\n",
    "        df_means = pd.concat([df_by_continent, df_global], axis = 0)\n",
    "        df_means = pd.merge(\n",
    "            df_means,\n",
    "            df_carbon_factors_grassland, \n",
    "            how = \"left\"\n",
    "        )\n",
    "        \n",
    "        \n",
    "        try:\n",
    "            df_growth, df_efs = get_factor_info(\n",
    "                df_means, \n",
    "                dict_cat_lndu_to_lucs, \n",
    "                dict_luc_to_cats_lndu,\n",
    "                iso\n",
    "            )\n",
    "\n",
    "            df_out_conv.append(df_efs)\n",
    "            df_out_sequestration.append(df_growth)\n",
    "        except:\n",
    "            print(f\"ISO {iso} failed\")\n",
    "            \n",
    "        \"\"\"\n",
    "        df_append = None\n",
    "        for field in fields_dat_cf:\n",
    "            \n",
    "            \n",
    "            df_cur = (\n",
    "                df_means[\n",
    "                    [\n",
    "                        field_count,\n",
    "                        field_type_forest,\n",
    "                        field\n",
    "                    ]\n",
    "                ]\n",
    "                .fillna(0)\n",
    "            )\n",
    "            \n",
    "            df_cur[field] = np.array(df_cur[field])*np.array(df_cur[field_count])/total_count\n",
    "            df_cur = sf.simple_df_agg(\n",
    "                df_cur.drop([field_count], axis = 1),\n",
    "                [field_type_forest],\n",
    "                {\n",
    "                    field: \"sum\"\n",
    "                }\n",
    "            )\n",
    "            \n",
    "            df_append = (\n",
    "                df_cur\n",
    "                if df_append is None\n",
    "                else pd.merge(df_append, df_cur)\n",
    "            )\n",
    "            \n",
    "            df_append[regions.field_iso] = iso\n",
    "            \n",
    "        df_out.append(df_append)\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "    df_out_conv = pd.concat(df_out_conv, axis = 0)\n",
    "    df_out_sequestration = pd.concat(df_out_sequestration, axis = 0)\n",
    "        \n",
    "    return df_out_conv, df_out_sequestration\n",
    "        \n",
    "\n",
    "\n",
    "df_efs_conv, df_efs_sequestration = get_average_forest_factors_by_iso(\n",
    "    df_lndu_climate_counts.dropna(subset = [field_luc_attr_name]),\n",
    "    df_carbon_factors_forest.drop([field_domain], axis = 1),\n",
    "    df_carbon_factors_grassland,\n",
    "    dict_luc_to_cats_lndu,\n",
    "    attr_kcc,\n",
    "    regions\n",
    ");\n",
    "\n",
    "\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "id": "a5f48697-201c-4b67-aed4-fff422cda464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time_period</th>\n",
       "      <th>ef_frst_sequestration_mangroves_kt_co2_ha</th>\n",
       "      <th>ef_frst_sequestration_primary_kt_co2_ha</th>\n",
       "      <th>ef_frst_sequestration_secondary_kt_co2_ha</th>\n",
       "      <th>iso_alpha_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>AFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>AFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>AFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>AFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.004724</td>\n",
       "      <td>0.001882</td>\n",
       "      <td>0.005430</td>\n",
       "      <td>AFG</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>ZWE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>ZWE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>ZWE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>ZWE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.004316</td>\n",
       "      <td>0.000025</td>\n",
       "      <td>0.004314</td>\n",
       "      <td>ZWE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5184 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    time_period  ef_frst_sequestration_mangroves_kt_co2_ha  \\\n",
       "0             0                                   0.004724   \n",
       "1             1                                   0.004724   \n",
       "2             2                                   0.004724   \n",
       "3             3                                   0.004724   \n",
       "4             4                                   0.004724   \n",
       "..          ...                                        ...   \n",
       "31           31                                   0.004316   \n",
       "32           32                                   0.004316   \n",
       "33           33                                   0.004316   \n",
       "34           34                                   0.004316   \n",
       "35           35                                   0.004316   \n",
       "\n",
       "    ef_frst_sequestration_primary_kt_co2_ha  \\\n",
       "0                                  0.001882   \n",
       "1                                  0.001882   \n",
       "2                                  0.001882   \n",
       "3                                  0.001882   \n",
       "4                                  0.001882   \n",
       "..                                      ...   \n",
       "31                                 0.000025   \n",
       "32                                 0.000025   \n",
       "33                                 0.000025   \n",
       "34                                 0.000025   \n",
       "35                                 0.000025   \n",
       "\n",
       "    ef_frst_sequestration_secondary_kt_co2_ha iso_alpha_3  \n",
       "0                                    0.005430         AFG  \n",
       "1                                    0.005430         AFG  \n",
       "2                                    0.005430         AFG  \n",
       "3                                    0.005430         AFG  \n",
       "4                                    0.005430         AFG  \n",
       "..                                        ...         ...  \n",
       "31                                   0.004314         ZWE  \n",
       "32                                   0.004314         ZWE  \n",
       "33                                   0.004314         ZWE  \n",
       "34                                   0.004314         ZWE  \n",
       "35                                   0.004314         ZWE  \n",
       "\n",
       "[5184 rows x 5 columns]"
      ]
     },
     "execution_count": 964,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_efs_sequestration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ac4100-638b-4279-9caf-c650e13e3c56",
   "metadata": {},
   "source": [
    "##  Build full data frames of output and merge in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1001,
   "id": "1e280d06-d2c1-4f48-bd15-521415bd7e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_out_conv = df_efs_conv.copy()\n",
    "df_out_seq = df_efs_sequestration.copy()\n",
    "\n",
    "\n",
    "# verify some checks\n",
    "df_out_conv_grouped = df_out_conv.groupby([regions.field_iso])\n",
    "df_out_seq_grouped = df_out_seq.groupby([regions.field_iso])\n",
    "regions_drop = []\n",
    "\n",
    "# first, check conversions\n",
    "for iso, df in df_out_conv_grouped:\n",
    "    arr = np.array(df[\"ef_lndu_conv_forests_primary_to_croplands_gg_co2_ha\"])\n",
    "    (\n",
    "        regions_drop.append(iso)\n",
    "        if arr.min() == 0\n",
    "        else None\n",
    "    )\n",
    "\n",
    "\n",
    "# then, check sequestrations\n",
    "for iso, df in df_out_seq_grouped:\n",
    "    arr = np.array(df[[x for x in df.columns if x.startswith(\"ef_frst\")]]).min(axis = 1)\n",
    "    (\n",
    "        regions_drop.append(iso)\n",
    "        if arr.min() == 0\n",
    "        else None\n",
    "    )\n",
    "\n",
    "# drop\n",
    "df_out_conv = (\n",
    "    df_out_conv[\n",
    "        ~df_out_conv[regions.field_iso].isin(regions_drop)\n",
    "    ]\n",
    "    .reset_index(drop = True)\n",
    ")\n",
    "df_out_seq = (\n",
    "    df_out_seq[\n",
    "        ~df_out_seq[regions.field_iso].isin(regions_drop)\n",
    "    ]\n",
    "    .reset_index(drop = True)\n",
    ")\n",
    "\n",
    "\n",
    "regions_succeeded = sorted(list(set(df_efs_conv[regions.field_iso]) - set(regions_drop)))\n",
    "regions_missing = [x for x in regions.all_isos if x not in regions_succeeded]\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "if len(regions_missing) > 0:\n",
    "    \n",
    "    df_out_conv = [df_out_conv]\n",
    "    df_out_seq = [df_out_seq]\n",
    "\n",
    "    for iso in regions_missing:\n",
    "\n",
    "        iso_closest = regions.get_closest_region(\n",
    "            iso,\n",
    "            regions_valid = regions_succeeded,\n",
    "            type_input = \"iso\",\n",
    "            type_return = \"iso\",\n",
    "        )\n",
    "\n",
    "        # pull dfs  & overwrite iso code - start with emission factors\n",
    "        df_ef_comp = df_efs_conv[df_efs_conv[regions.field_iso].isin([iso_closest])].copy()\n",
    "        df_ef_comp[regions.field_iso] = iso\n",
    "        df_out_conv.append(df_ef_comp)\n",
    "\n",
    "        # add in sequestration\n",
    "        df_ef_seq = df_efs_sequestration[df_efs_sequestration[regions.field_iso].isin([iso_closest])].copy()\n",
    "        df_ef_seq[regions.field_iso] = iso\n",
    "        df_out_seq.append(df_ef_seq)\n",
    "\n",
    "    df_out_conv = pd.concat(df_out_conv, axis = 0)\n",
    "    df_out_seq = pd.concat(df_out_seq, axis = 0)\n",
    "\n",
    "fields_force_zero = [\n",
    "    x for x in df_out_conv.columns\n",
    "    if x.startswith(\"ef_lndu_conv_forests\")\n",
    "    & (\"_to_forests\" in x)\n",
    "]\n",
    "\n",
    "df_out_conv[fields_force_zero] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1002,
   "id": "a1c6cafd-760e-4525-bc0e-8451be2bb72c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_out_conv.to_csv(\n",
    "    sa.fp_csv_lndu_ef_conversion_co2,\n",
    "    index = None,\n",
    "    encoding = \"UTF-8\"\n",
    ")\n",
    "\n",
    "df_out_seq.to_csv(\n",
    "    sa.fp_csv_lndu_ef_forest_sequestration_co2,\n",
    "    index = None,\n",
    "    encoding = \"UTF-8\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1004,
   "id": "4b4ce494-582b-42a6-a1aa-c07724f91657",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_croplands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_croplands_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_croplands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_croplands_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_croplands_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_croplands_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_forests_primary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_croplands_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_forests_primary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_croplands_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_forests_secondary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_croplands_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_forests_secondary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_croplands_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_grasslands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_croplands_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_grasslands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_croplands_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_other_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_croplands_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_other_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_croplands_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_settlements_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_croplands_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_settlements_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_croplands_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_wetlands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_croplands_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_croplands_to_wetlands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_croplands_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_croplands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_mangroves_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_croplands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_mangroves_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_mangroves_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_mangroves_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_forests_primary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_mangroves_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_forests_primary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_mangroves_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_forests_secondary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_mangroves_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_forests_secondary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_mangroves_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_grasslands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_mangroves_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_grasslands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_mangroves_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_other_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_mangroves_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_other_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_mangroves_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_settlements_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_mangroves_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_settlements_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_mangroves_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_wetlands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_mangroves_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_mangroves_to_wetlands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_mangroves_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_croplands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_primary_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_croplands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_primary_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_primary_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_primary_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_forests_primary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_primary_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_forests_primary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_primary_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_forests_secondary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_primary_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_forests_secondary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_primary_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_grasslands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_primary_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_grasslands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_primary_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_other_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_primary_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_other_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_primary_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_settlements_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_primary_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_settlements_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_primary_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_wetlands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_primary_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_primary_to_wetlands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_primary_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_croplands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_secondary_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_croplands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_secondary_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_secondary_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_secondary_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_forests_primary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_secondary_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_forests_primary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_secondary_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_forests_secondary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_secondary_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_forests_secondary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_secondary_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_grasslands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_secondary_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_grasslands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_secondary_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_other_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_secondary_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_other_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_secondary_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_settlements_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_secondary_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_settlements_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_secondary_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_wetlands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_forests_secondary_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_forests_secondary_to_wetlands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_forests_secondary_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_croplands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_grasslands_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_croplands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_grasslands_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_grasslands_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_grasslands_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_forests_primary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_grasslands_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_forests_primary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_grasslands_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_forests_secondary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_grasslands_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_forests_secondary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_grasslands_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_grasslands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_grasslands_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_grasslands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_grasslands_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_other_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_grasslands_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_other_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_grasslands_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_settlements_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_grasslands_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_settlements_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_grasslands_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_wetlands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_grasslands_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_grasslands_to_wetlands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_grasslands_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_croplands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_other_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_croplands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_other_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_other_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_other_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_forests_primary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_other_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_forests_primary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_other_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_forests_secondary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_other_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_forests_secondary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_other_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_grasslands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_other_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_grasslands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_other_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_other_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_other_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_other_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_other_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_settlements_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_other_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_settlements_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_other_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_wetlands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_other_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_other_to_wetlands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_other_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_croplands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_settlements_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_croplands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_settlements_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_settlements_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_settlements_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_forests_primary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_settlements_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_forests_primary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_settlements_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_forests_secondary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_settlements_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_forests_secondary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_settlements_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_grasslands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_settlements_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_grasslands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_settlements_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_other_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_settlements_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_other_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_settlements_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_settlements_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_settlements_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_settlements_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_settlements_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_wetlands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_settlements_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_settlements_to_wetlands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_settlements_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_croplands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_wetlands_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_croplands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_wetlands_to_croplands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_wetlands_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_forests_mangroves_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_wetlands_to_forests_mangroves_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_forests_primary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_wetlands_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_forests_primary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_wetlands_to_forests_primary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_forests_secondary_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_wetlands_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_forests_secondary_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_wetlands_to_forests_secondary_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_grasslands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_wetlands_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_grasslands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_wetlands_to_grasslands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_other_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_wetlands_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_other_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_wetlands_to_other_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_settlements_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_wetlands_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_settlements_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_wetlands_to_settlements_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_wetlands_gg_co2_ha/input_to_sisepuede/historical/ef_lndu_conv_wetlands_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_lndu_conv_wetlands_to_wetlands_gg_co2_ha/input_to_sisepuede/projected/ef_lndu_conv_wetlands_to_wetlands_gg_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_frst_sequestration_mangroves_kt_co2_ha/input_to_sisepuede/historical/ef_frst_sequestration_mangroves_kt_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_frst_sequestration_mangroves_kt_co2_ha/input_to_sisepuede/projected/ef_frst_sequestration_mangroves_kt_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_frst_sequestration_primary_kt_co2_ha/input_to_sisepuede/historical/ef_frst_sequestration_primary_kt_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_frst_sequestration_primary_kt_co2_ha/input_to_sisepuede/projected/ef_frst_sequestration_primary_kt_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_frst_sequestration_secondary_kt_co2_ha/input_to_sisepuede/historical/ef_frst_sequestration_secondary_kt_co2_ha.csv'\n",
      "DataFrame successfully written to '/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/ef_frst_sequestration_secondary_kt_co2_ha/input_to_sisepuede/projected/ef_frst_sequestration_secondary_kt_co2_ha.csv'\n"
     ]
    }
   ],
   "source": [
    "years_hist = list(range(2015, 2020))\n",
    "tup = repo.write_from_df(\n",
    "    df_out_conv,\n",
    "    years_hist,\n",
    "    #periods_write = [\"projected\"],\n",
    "    write_q = True\n",
    ")\n",
    "\n",
    "tup = repo.write_from_df(\n",
    "    df_out_seq,\n",
    "    years_hist,\n",
    "    #periods_write = [\"projected\"],\n",
    "    write_q = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd489ab-a94b-489c-8aa7-241ef2153794",
   "metadata": {},
   "outputs": [],
   "source": [
    "repo.read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1000,
   "id": "9246f12e-770b-46b4-b035-b20a55fe88df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ef_lndu_conv_forests_mangroves_to_forests_mangroves_gg_co2_ha',\n",
       " 'ef_lndu_conv_forests_mangroves_to_forests_primary_gg_co2_ha',\n",
       " 'ef_lndu_conv_forests_mangroves_to_forests_secondary_gg_co2_ha',\n",
       " 'ef_lndu_conv_forests_primary_to_forests_mangroves_gg_co2_ha',\n",
       " 'ef_lndu_conv_forests_primary_to_forests_primary_gg_co2_ha',\n",
       " 'ef_lndu_conv_forests_primary_to_forests_secondary_gg_co2_ha',\n",
       " 'ef_lndu_conv_forests_secondary_to_forests_mangroves_gg_co2_ha',\n",
       " 'ef_lndu_conv_forests_secondary_to_forests_primary_gg_co2_ha',\n",
       " 'ef_lndu_conv_forests_secondary_to_forests_secondary_gg_co2_ha']"
      ]
     },
     "execution_count": 1000,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\n",
    "    x for x in df_out_conv.columns\n",
    "    if x.startswith(\"ef_lndu_conv_forests\")\n",
    "    & (\"_to_forests\" in x)\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 994,
   "id": "f1513e54-895e-4504-bb62-6baa739aab4d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'setup_analysis' from '/Users/jsyme/Documents/Projects/git_jbus/lac_decarbonization/python/setup_analysis.py'>"
      ]
     },
     "execution_count": 994,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(sa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe2a1a2-c940-499a-a2ad-002ac0dceb44",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9faeb8c9-ec66-4472-9a70-3c0b333c705d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5051a6c-41ff-4cec-94aa-98e69696915c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5122c758-6914-445c-8e71-65af58d32934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fdfe887-0363-43ae-864b-892056862b1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ccca345-827a-4d13-9b5f-14427c75c6f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de25c3cd-cb0b-45c7-8f8c-7363306b60f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2a37de-cab0-4c05-a1b7-9d7c57f15626",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5af8f1-b72e-45e2-bd4f-386710b5e6cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4695297e-7495-415e-97e6-7186e802966b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d203eace-5f66-4752-8b95-9332413e5f24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "2bf58caa-d318-4a83-b89e-5ceb78ff05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = pd.read_csv(\"/Users/jsyme/Documents/Projects/git_jbus/sisepuede_data/AFOLU/frac_lndu_initial_croplands/row_data/Land_cover_data/items_classification.csv\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8de9bd8a-975d-4755-b951-383093b4e88a",
   "metadata": {},
   "source": [
    "# Build overlay of land use type by climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4c7d5681-9a30-47ab-963c-8a8e40da0c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, os.path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import model_attributes as ma\n",
    "from attribute_table import AttributeTable\n",
    "import setup_analysis as sa\n",
    "import support_classes as sc\n",
    "import support_functions as sf\n",
    "import importlib\n",
    "import time\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import geopandas as gpd\n",
    "import rioxarray as rx\n",
    "import itertools\n",
    "import model_afolu as mafl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11e7fe7-6fba-4b4a-a3bd-acb35384b009",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_data = \"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/AFOLU\"\n",
    "# set names \n",
    "fn_climates = \"kc_1984_2013.tif\"\n",
    "fn_countries = \"WB_countries_Admin0_10m\"\n",
    "fn_cw = \"values_info_with_cw_kc_1984_2013.csv\"\n",
    "\n",
    "fp_climates = os.path.join(dir_data, fn_climates)\n",
    "fp_lu = os.path.join(dir_data, \"GlcShare_v10_Dominant\", \"glc_shv10_DOM.Tif\")\n",
    "fp_countries = os.path.join(dir_data, fn_countries, f\"{fn_countries}.shp\")\n",
    "fp_cw = os.path.join(dir_data, fn_cw)\n",
    "\n",
    "model_afolu = mafl.AFOLU(sa.model_attributes)\n",
    "regions = sc.Regions(sa.model_attributes)\n",
    "time_periods = sc.TimePeriods(sa.model_attributes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f7c74e0-aa49-467b-ab25-309b304a190d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert geotiff to dataframe\n",
    "rx_array = rx.open_rasterio(fp_climates)\n",
    "df_climates = rx_array[0].to_pandas()\n",
    "# retrieve climate categories\n",
    "df_climate_cats = pd.read_csv(fp_cw, sep = \",\")\n",
    "\n",
    "\n",
    "# convert geotiff to dataframe\n",
    "rx_array = rx.open_rasterio(fp_lu)\n",
    "df_lu = rx_array[0].to_pandas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b88a5d05-e36a-4b36-a48e-5ee470f97d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_climate_and_lu_comparison(\n",
    "    df_climates: pd.DataFrame,\n",
    "    df_lu: pd.DataFrame,\n",
    "    digits: int = 6,\n",
    ") -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Check if tiff-derived climate and land use data frames have the\n",
    "        same shared indices based on rounding to error epsilon\n",
    "    \"\"\"\n",
    "    return_reduced = True\n",
    "    \n",
    "    # check number of rows\n",
    "    m_a = len(df_climates.index)\n",
    "    m_b = len(df_lu.index)\n",
    "    m_min = min(m_a, m_b)\n",
    "\n",
    "    # check index\n",
    "    vec_ind_diff_rows = np.round(\n",
    "        np.array(df_climates.index)[0:m_min].astype(float), \n",
    "        decimals = digits\n",
    "    )\n",
    "    vec_ind_diff_rows -= np.round(\n",
    "        np.array(df_lu.index)[0:m_min].astype(float), \n",
    "        decimals = digits\n",
    "    )\n",
    "    \n",
    "    return_reduced &= (np.abs(vec_ind_diff_rows).max() < 10**(-digits))\n",
    "    \n",
    "    \n",
    "    # check number of columns\n",
    "    n_a = len(df_climates.columns)\n",
    "    n_b = len(df_lu.columns)\n",
    "    n_min = min(n_a, n_b)\n",
    "\n",
    "    # check index\n",
    "    vec_ind_diff_cols = np.round(\n",
    "        np.array(df_climates.columns)[0:n_min].astype(float), \n",
    "        decimals = digits\n",
    "    )\n",
    "    vec_ind_diff_cols -= np.round(\n",
    "        np.array(df_lu.columns)[0:n_min].astype(float), \n",
    "        decimals = digits\n",
    "    )\n",
    "    \n",
    "    \n",
    "    return_reduced &= (np.abs(vec_ind_diff_cols).max() < 10**(-digits))\n",
    "    if not return_reduced:\n",
    "        return None\n",
    "    \n",
    "    # modify \n",
    "    df_climates = df_climates.iloc[0:m_min, 0:n_min]\n",
    "    df_climates.index = np.round(\n",
    "        np.array(df_climates.index)[0:m_min].astype(float),\n",
    "        decimals = digits\n",
    "    )\n",
    "    df_climates.columns = np.round(\n",
    "        np.array(df_climates.columns)[0:n_min].astype(float),\n",
    "        decimals = digits\n",
    "    )\n",
    "    \n",
    "    # modify \n",
    "    df_lu = df_lu.iloc[0:m_min, 0:n_min]\n",
    "    df_lu.index = np.round(\n",
    "        np.array(df_lu.index)[0:m_min].astype(float),\n",
    "        decimals = digits\n",
    "    )\n",
    "    df_lu.columns = np.round(\n",
    "        np.array(df_lu.columns)[0:n_min].astype(float),\n",
    "        decimals = digits\n",
    "    )\n",
    "    \n",
    "    out = (\n",
    "        df_climates,\n",
    "        df_lu\n",
    "    )\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "df_climates, df_lu = check_climate_and_lu_comparison(\n",
    "    df_climates,\n",
    "    df_lu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968d4066-382c-4f29-9f87-3d49c15eb959",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0df7dece-e7f9-4098-a356-0cc31f9806ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/AFOLU/kcc_cells_merged_to_country/kcc_coords_index.csv\", nrows = 100)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2b99d4-058d-4de5-bbcf-cbea548a998a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_out = \"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/AFOLU/kcc_luc_merged_to_country\"\n",
    "os.path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af68917e-cb7b-4672-980b-1a39e3f1b23f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "28e662d5-c3e0-4dd3-9954-a505fe7a2603",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_grid_data_from_index(\n",
    "    df_inds: pd.DataFrame,\n",
    "    df_gridded_values: pd.DataFrame,\n",
    "    field_value_new: str,\n",
    "    digits: int = 6,\n",
    "    field_x: str = field_lon,\n",
    "    field_y: str = field_lat,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For a given long data frame df_inds of lat/lon, retrieve associated\n",
    "        values from a gridded data frame with lat/lon\n",
    "        \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - df_inds: input DataFrame containing values and indices by lat/lon centroid, \n",
    "        including x, y, and value\n",
    "    - df_grid: DataFrame containing lat as row index and lon as column\n",
    "        index. Values in df_inds[field_lat] and df_inds[field_lon] are \n",
    "        matched to y and x indices, respectively, rounding to match using\n",
    "        digits_round_index.\n",
    "    - field_value_new: field name to use for new value in df\n",
    "    \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - digits: number of digits to use for rounding lat/lon\n",
    "    - field_x: field name in df_inds containing x (lon) coordinates (column names\n",
    "        in df_gridded_values)\n",
    "    - field_y: field name in df_inds containing y (lon) coordinates (row index \n",
    "        names in df_gridded_values)\n",
    "    \"\"\"\n",
    "\n",
    "    # get latitude and vector longitudes\n",
    "    lat = np.round(df_inds[field_lat].iloc[0], decimals = digits)\n",
    "    lons = np.round(np.array(df_inds[field_lon]), decimals = digits)\n",
    "    # \n",
    "    vec_x = np.array(df_gridded_values.loc[lat][lons])\n",
    "    df_inds[field_value_new] = vec_x\n",
    "    \n",
    "    return df_inds\n",
    "\n",
    "\n",
    "\n",
    "def get_gridded_data_by_index_data_frame(\n",
    "    df_grid: pd.DataFrame,\n",
    "    df_index_data: pd.DataFrame,\n",
    "    digits_round_index: int = 6,\n",
    "    field_lat: str = \"y\",\n",
    "    field_lon: str = \"x\",\n",
    "    field_value_index: str = \"value\",\n",
    "    field_value_index_out: str = field_kcc,\n",
    "    field_value_new: str = field_luc,\n",
    "    missing_val: int = -999,\n",
    "    return_df: bool = True,\n",
    ") -> Union[pd.DataFrame, np.ndarray, None]:\n",
    "    \"\"\"\n",
    "    Using broken (by country) KCC data frames, get land use categories by\n",
    "        cell centroid (assumes KCC and LU grids have same centroids)\n",
    "        \n",
    "    \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - df_grid: DataFrame containing lat as row index and lon as column\n",
    "        index. Values in df_index_data[field_lat] and df_index_data[field_lon] are \n",
    "        matched to y and x indices, respectively, rounding to match using\n",
    "        digits_round_index.\n",
    "    - df_index_data: input DataFrame containing KCC indices by lat/lon centroid, \n",
    "        including x, y, and value\n",
    "        \n",
    "    \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - digits_round_index: number of digits to use for rounding lat/lon to look\n",
    "        values in land use\n",
    "    - field_lat: field containing latitude\n",
    "    - field_lon: field_containing longitude\n",
    "    - field_value_index: field in df_index_data containing data values\n",
    "    - field_value_index_out: new field name for data field stored in df_index_data\n",
    "    - field_value_new: new field name for data merged in from df_grid\n",
    "    - missing_val: value to use if lat/lon not found in df_grid\n",
    "    - return_df: return a dataframe merging the new column to df_index_data? if False,\n",
    "        returns ordered column vector only\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    # validate inputs\n",
    "    \n",
    "    return_none = False\n",
    "    return_none |= not isinstance(df_grid, pd.DataFrame)\n",
    "    return_none |= not isinstance(df_index_data, pd.DataFrame)\n",
    "    #\n",
    "    # check grid matching here\n",
    "    #\n",
    "    if return_none:\n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    # group Kopen Climate Classification data frame by latitude\n",
    "    dfg = (\n",
    "        df_index_data\n",
    "        .groupby([field_lat])\n",
    "    )\n",
    "\n",
    "    df_out = []\n",
    "    for y, df in dfg:\n",
    "        df_out.append(\n",
    "            get_grid_data_from_index(\n",
    "                df,\n",
    "                df_grid,\n",
    "                field_value_new,\n",
    "                digits = digits_round_index,\n",
    "                field_x = field_lon,\n",
    "                field_y = field_lat,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "    df_out = (\n",
    "        pd.concat(df_out, axis = 0)\n",
    "        .sort_index()\n",
    "        .rename(columns = {field_value_index: field_value_index_out})\n",
    "    )\n",
    "    \n",
    "    \"\"\"\n",
    "    df_out = dfg.apply(\n",
    "        get_grid_data_from_index,\n",
    "        df_grid,\n",
    "        field_luc,\n",
    "        digits = digits_round_index,\n",
    "        field_x = field_lon,\n",
    "        field_y = field_lat,\n",
    "    )\n",
    "    \"\"\";\n",
    "\n",
    "    return df_out\n",
    "\n",
    "\n",
    "\n",
    "def merge_gridded_data_to_kcc_file(\n",
    "    df_grid: pd.DataFrame,\n",
    "    fp_coords: str,\n",
    "    fp_kcc_data: str,\n",
    "    field_data_value: str,\n",
    "    field_index: str = field_index,\n",
    "    field_x: str = \"x\",\n",
    "    field_y: str = \"y\",\n",
    "    header_coords: [List[str], None] = None,\n",
    "    output_as_index: bool = True,\n",
    "    **kwargs\n",
    ") -> Union[pd.DataFrame, None]:\n",
    "    \"\"\"\n",
    "    Retrieve coordinates associated with indices stored in KCC files\n",
    "    \n",
    "    Function Arguments\n",
    "    ------------------\n",
    "    - df_grid: data frame containing gridded data\n",
    "    - fp_coords: file path to coordinates\n",
    "    - fp_kcc_data: file path to data containing KCC indexed by fp_coords\n",
    "    - field_data_value: field to use for new data value \n",
    "    \n",
    "    Keyword Arguments\n",
    "    -----------------\n",
    "    - field_index: field in input file fp_coord\n",
    "    - header_coords: list giving columns. If None, reads from fp_coords\n",
    "    - output_as_index: output DataFrame indexes by field_index? If False,\n",
    "        includes coordinates\n",
    "    - **kwargs: passed to get_gridded_data_by_index_data_frame()\n",
    "    \"\"\"\n",
    "    # get header\n",
    "    header_coords = (\n",
    "        list(pd.read_csv(fp_coords, nrows = 0).columns)\n",
    "        if header_coords is None\n",
    "        else header_coords\n",
    "    )\n",
    "    \n",
    "    \n",
    "    df_get = pd.read_csv(fp_kcc_data)\n",
    "        \n",
    "    # get indices\n",
    "    vec_index = np.array(df_get[field_index]).astype(int)\n",
    "    min_ind = min(vec_index)\n",
    "    max_ind = max(vec_index)\n",
    "    vec_extract = vec_index - min_ind\n",
    "        \n",
    "    # read array from file\n",
    "    df_lat_lon = sf.read_array_from_file(\n",
    "        fp_coords,\n",
    "        len(header_coords),\n",
    "        min_ind, \n",
    "        max_ind,\n",
    "        skip_header = True,\n",
    "    )\n",
    "    df_lat_lon = df_lat_lon[vec_extract, :]\n",
    "    df_lat_lon = pd.DataFrame(df_lat_lon, columns = header_coords)\n",
    "    (\n",
    "        df_lat_lon.drop([field_data_value], axis = 1, inplace = True)\n",
    "        if field_data_value in df_lat_lon.columns\n",
    "        else None\n",
    "    )\n",
    "    \n",
    "    # get land use classes associated with the rows in these indices--sorted by index\n",
    "    df_grid_out = get_gridded_data_by_index_data_frame(\n",
    "        df_grid,\n",
    "        df_lat_lon,\n",
    "        field_value_new = field_data_value,\n",
    "        field_lat = field_y,\n",
    "        field_lon = field_x,\n",
    "        **kwargs\n",
    "    )\n",
    "    \n",
    "    df_grid_out = df_lat_lon.join(df_grid_out[[field_data_value]])\n",
    "    if output_as_index:\n",
    "        (\n",
    "            df_grid_out\n",
    "            .drop(\n",
    "                [field_x, field_y],\n",
    "                axis = 1, \n",
    "                inplace = True\n",
    "            )\n",
    "        )\n",
    "        df_grid_out = pd.concat(\n",
    "            [\n",
    "                df_get[[field_index]],\n",
    "                df_grid_out\n",
    "            ], \n",
    "            axis = 1\n",
    "        )\n",
    "    \n",
    "    return df_grid_out\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e5d211-3ff3-4b1f-bab7-7677f1a932f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce84349a-0f25-4342-9875-d05ff9ba1227",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "id": "3f8c10e7-9b24-4890-a73d-8c722857a977",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kcc_and_lndusecat_by_iso_BRA.csv'"
      ]
     },
     "execution_count": 436,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_read = os.path.join(dir_data, \"kcc_and_lndusecat_country\")\n",
    "regex_match = re.compile(\"kcc_and_lndusecat_by_iso_(\\D*).csv\")\n",
    "#dir_read = os.path.join(dir_data, \"KCC_LUC_aggs_by_country\")\n",
    "#regex_match = re.compile(\"kcc_and_lndusecat_by_iso(\\D*)_agg.csv\")\n",
    "fls_read = sorted([x for x in os.listdir(dir_read) if regex_match.match(x) is not None])\n",
    "fls_read[26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "1e6fdfe1-39f8-40e9-95fd-09352592f5fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_coords = \"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/AFOLU/kcc_and_lndusecat_country/index_right_coords.csv\"\n",
    " \n",
    "t0 = time.time()\n",
    "df_ord = merge_gridded_data_to_kcc_file(\n",
    "    df_lu,\n",
    "    fp_coords,\n",
    "    os.path.join(dir_read, fls_read[26]),\n",
    "    field_luc,\n",
    "    output_as_index = False,\n",
    ")\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40662345-3aea-4d8c-894c-e04555080255",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "id": "f28498e6-6857-4c76-80c5-4b9236ca540a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>value</th>\n",
       "      <th>luc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9915951</th>\n",
       "      <td>-64.445833</td>\n",
       "      <td>-6.104167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916015</th>\n",
       "      <td>-64.495833</td>\n",
       "      <td>-6.104167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916028</th>\n",
       "      <td>-64.429167</td>\n",
       "      <td>-6.104167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916041</th>\n",
       "      <td>-64.479167</td>\n",
       "      <td>-6.104167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9916054</th>\n",
       "      <td>-64.404167</td>\n",
       "      <td>-6.104167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917580</th>\n",
       "      <td>-64.420833</td>\n",
       "      <td>-6.195833</td>\n",
       "      <td>130.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917582</th>\n",
       "      <td>-64.412500</td>\n",
       "      <td>-6.195833</td>\n",
       "      <td>130.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917590</th>\n",
       "      <td>-64.404167</td>\n",
       "      <td>-6.195833</td>\n",
       "      <td>130.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917591</th>\n",
       "      <td>-64.462500</td>\n",
       "      <td>-6.195833</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9917612</th>\n",
       "      <td>-64.445833</td>\n",
       "      <td>-6.195833</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>144 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x         y  value  luc\n",
       "9915951 -64.445833 -6.104167  110.0    4\n",
       "9916015 -64.495833 -6.104167  110.0    4\n",
       "9916028 -64.429167 -6.104167  110.0    4\n",
       "9916041 -64.479167 -6.104167  110.0    4\n",
       "9916054 -64.404167 -6.104167  110.0    4\n",
       "...            ...       ...    ...  ...\n",
       "9917580 -64.420833 -6.195833  130.0   11\n",
       "9917582 -64.412500 -6.195833  130.0    4\n",
       "9917590 -64.404167 -6.195833  130.0    4\n",
       "9917591 -64.462500 -6.195833  110.0    4\n",
       "9917612 -64.445833 -6.195833  110.0    4\n",
       "\n",
       "[144 rows x 4 columns]"
      ]
     },
     "execution_count": 440,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ord[\n",
    "    (df_ord[\"y\"] < -6.1)\n",
    "    & (df_ord[\"y\"] > -6.2)\n",
    "    & (df_ord[\"x\"] < -64.4)\n",
    "    & (df_ord[\"x\"] > -64.5)\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "0597d74e-b7f7-4fda-b353-ebbed0f69336",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "import re\n",
    "importlib.reload(sf)\n",
    "\n",
    "    \n",
    "# get files \n",
    "\n",
    "#dir_read = os.path.join(dir_data, \"kcc_cells_merged_to_country\")\n",
    "#regex_match = re.compile(\"kcc_and_lndusecat_by_iso_(\\D*).csv\")\n",
    "dir_read = os.path.join(dir_data, \"KCC_LUC_aggs_by_country\")\n",
    "regex_match = re.compile(\"kcc_and_lndusecat_by_iso(\\D*)_agg.csv\")\n",
    "fls_read = sorted([x for x in os.listdir(dir_read) if regex_match.match(x) is not None])\n",
    "\n",
    "\n",
    "for i, fl in enumerate(fls_read):\n",
    "    \n",
    "    fp_cur = os.path.join(dir_read, fl)\n",
    "    fp_new = os.path.join(dir_read, fl.replace(\"kcc_by_country\", \"kcc_and_lndusecat_by_iso\"))\n",
    "    \n",
    "    os.rename(fp_cur, fp_new)\n",
    "\"\"\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "25f68e32-7257-4720-b7c6-fdc075683063",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting ISO ABW...\n",
      "Country 'ABW' complete in 26.36 seconds.\n",
      "\n",
      "\n",
      "Starting ISO AFG...\n",
      "Country 'AFG' complete in 64.4 seconds.\n",
      "\n",
      "\n",
      "Starting ISO AGO...\n",
      "Country 'AGO' complete in 63.34 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ALB...\n",
      "Country 'ALB' complete in 31.23 seconds.\n",
      "\n",
      "\n",
      "Starting ISO AND...\n",
      "Country 'AND' complete in 14.18 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ARE...\n",
      "Country 'ARE' complete in 31.54 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ARG...\n",
      "Country 'ARG' complete in 76.61 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ARM...\n",
      "Country 'ARM' complete in 26.69 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ASM...\n",
      "Country 'ASM' complete in 30.52 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ATG...\n",
      "Country 'ATG' complete in 21.72 seconds.\n",
      "\n",
      "\n",
      "Starting ISO AUS...\n",
      "Country 'AUS' complete in 122.35 seconds.\n",
      "\n",
      "\n",
      "Starting ISO AUT...\n",
      "Country 'AUT' complete in 27.24 seconds.\n",
      "\n",
      "\n",
      "Starting ISO AZE...\n",
      "Country 'AZE' complete in 30.02 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BDI...\n",
      "Country 'BDI' complete in 27.48 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BEL...\n",
      "Country 'BEL' complete in 22.6 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BEN...\n",
      "Country 'BEN' complete in 36.62 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BFA...\n",
      "Country 'BFA' complete in 35.74 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BGD...\n",
      "Country 'BGD' complete in 42.66 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BGR...\n",
      "Country 'BGR' complete in 28.6 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BHR...\n",
      "Country 'BHR' complete in 18.72 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BHS...\n",
      "Country 'BHS' complete in 43.18 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BIH...\n",
      "Country 'BIH' complete in 28.03 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BLR...\n",
      "Country 'BLR' complete in 42.09 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BLZ...\n",
      "Country 'BLZ' complete in 27.72 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BMU...\n",
      "Country 'BMU' complete in 16.61 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BOL...\n",
      "Country 'BOL' complete in 61.71 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BRA...\n",
      "Country 'BRA' complete in 145.82 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BRB...\n",
      "Country 'BRB' complete in 21.36 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BRN...\n",
      "Country 'BRN' complete in 24.16 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BTN...\n",
      "Country 'BTN' complete in 24.86 seconds.\n",
      "\n",
      "\n",
      "Starting ISO BWA...\n",
      "Country 'BWA' complete in 49.91 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CAF...\n",
      "Country 'CAF' complete in 45.85 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CAN...\n",
      "Country 'CAN' complete in 280.24 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CHE...\n",
      "Country 'CHE' complete in 24.75 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CHL...\n",
      "Country 'CHL' complete in 81.71 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CHN...\n",
      "Country 'CHN' complete in 229.26 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CIV...\n",
      "Country 'CIV' complete in 38.64 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CMR...\n",
      "Country 'CMR' complete in 50.89 seconds.\n",
      "\n",
      "\n",
      "Starting ISO COD...\n",
      "Country 'COD' complete in 75.38 seconds.\n",
      "\n",
      "\n",
      "Starting ISO COG...\n",
      "Country 'COG' complete in 44.5 seconds.\n",
      "\n",
      "\n",
      "Starting ISO COL...\n",
      "Country 'COL' complete in 70.18 seconds.\n",
      "\n",
      "\n",
      "Starting ISO COM...\n",
      "Country 'COM' complete in 26.93 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CPV...\n",
      "Country 'CPV' complete in 26.72 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CRI...\n",
      "Country 'CRI' complete in 35.19 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CUB...\n",
      "Country 'CUB' complete in 31.9 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CUW...\n",
      "Country 'CUW' complete in 21.37 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CYM...\n",
      "Country 'CYM' complete in 20.52 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CYP...\n",
      "Country 'CYP' complete in 20.33 seconds.\n",
      "\n",
      "\n",
      "Starting ISO CZE...\n",
      "Country 'CZE' complete in 26.27 seconds.\n",
      "\n",
      "\n",
      "Starting ISO DEU...\n",
      "Country 'DEU' complete in 61.31 seconds.\n",
      "\n",
      "\n",
      "Starting ISO DJI...\n",
      "Country 'DJI' complete in 25.21 seconds.\n",
      "\n",
      "\n",
      "Starting ISO DMA...\n",
      "Country 'DMA' complete in 21.02 seconds.\n",
      "\n",
      "\n",
      "Starting ISO DNK...\n",
      "Country 'DNK' complete in 27.08 seconds.\n",
      "\n",
      "\n",
      "Starting ISO DOM...\n",
      "Country 'DOM' complete in 27.35 seconds.\n",
      "\n",
      "\n",
      "Starting ISO DZA...\n",
      "Country 'DZA' complete in 104.87 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ECU...\n",
      "Country 'ECU' complete in 39.98 seconds.\n",
      "\n",
      "\n",
      "Starting ISO EGY...\n",
      "Country 'EGY' complete in 61.87 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ERI...\n",
      "Country 'ERI' complete in 36.27 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ESP...\n",
      "Country 'ESP' complete in 93.64 seconds.\n",
      "\n",
      "\n",
      "Starting ISO EST...\n",
      "Country 'EST' complete in 20.08 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ETH...\n",
      "Country 'ETH' complete in 52.09 seconds.\n",
      "\n",
      "\n",
      "Starting ISO FIN...\n",
      "Country 'FIN' complete in 88.59 seconds.\n",
      "\n",
      "\n",
      "Starting ISO FJI...\n",
      "Country 'FJI' complete in 45.05 seconds.\n",
      "\n",
      "\n",
      "Starting ISO FRO...\n",
      "Country 'FRO' complete in 13.98 seconds.\n",
      "\n",
      "\n",
      "Starting ISO FSM...\n",
      "Country 'FSM' complete in 33.0 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GAB...\n",
      "Country 'GAB' complete in 38.5 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GBR...\n",
      "Country 'GBR' complete in 79.63 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GEO...\n",
      "Country 'GEO' complete in 25.77 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GHA...\n",
      "Country 'GHA' complete in 37.95 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GIB...\n",
      "Country 'GIB' complete in 14.89 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GIN...\n",
      "Country 'GIN' complete in 35.16 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GMB...\n",
      "Country 'GMB' complete in 22.06 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GNB...\n",
      "Country 'GNB' complete in 24.73 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GNQ...\n",
      "Country 'GNQ' complete in 34.44 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GRC...\n",
      "Country 'GRC' complete in 49.26 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GRD...\n",
      "Country 'GRD' complete in 21.17 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GRL...\n",
      "Country 'GRL' complete in 143.18 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GTM...\n",
      "Country 'GTM' complete in 32.11 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GUM...\n",
      "Country 'GUM' complete in 20.65 seconds.\n",
      "\n",
      "\n",
      "Starting ISO GUY...\n",
      "Country 'GUY' complete in 39.32 seconds.\n",
      "\n",
      "\n",
      "Starting ISO HKG...\n",
      "Country 'HKG' complete in 20.24 seconds.\n",
      "\n",
      "\n",
      "Starting ISO HND...\n",
      "Country 'HND' complete in 32.54 seconds.\n",
      "\n",
      "\n",
      "Starting ISO HRV...\n",
      "Country 'HRV' complete in 34.87 seconds.\n",
      "\n",
      "\n",
      "Starting ISO HTI...\n",
      "Country 'HTI' complete in 24.96 seconds.\n",
      "\n",
      "\n",
      "Starting ISO HUN...\n",
      "Country 'HUN' complete in 28.71 seconds.\n",
      "\n",
      "\n",
      "Starting ISO IDN...\n",
      "Country 'IDN' complete in 67.26 seconds.\n",
      "\n",
      "\n",
      "Starting ISO IMN...\n",
      "Country 'IMN' complete in 11.18 seconds.\n",
      "\n",
      "\n",
      "Starting ISO IND...\n",
      "Country 'IND' complete in 134.55 seconds.\n",
      "\n",
      "\n",
      "Starting ISO IRL...\n",
      "Country 'IRL' complete in 33.62 seconds.\n",
      "\n",
      "\n",
      "Starting ISO IRN...\n",
      "Country 'IRN' complete in 87.05 seconds.\n",
      "\n",
      "\n",
      "Starting ISO IRQ...\n",
      "Country 'IRQ' complete in 54.02 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ISL...\n",
      "Country 'ISL' complete in 30.52 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ISR...\n",
      "Country 'ISR' complete in 32.23 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ITA...\n",
      "Country 'ITA' complete in 73.81 seconds.\n",
      "\n",
      "\n",
      "Starting ISO JAM...\n",
      "Country 'JAM' complete in 21.5 seconds.\n",
      "\n",
      "\n",
      "Starting ISO JOR...\n",
      "Country 'JOR' complete in 34.86 seconds.\n",
      "\n",
      "\n",
      "Starting ISO JPN...\n",
      "Country 'JPN' complete in 118.02 seconds.\n",
      "\n",
      "\n",
      "Starting ISO KAZ...\n",
      "Country 'KAZ' complete in 108.48 seconds.\n",
      "\n",
      "\n",
      "Starting ISO KEN...\n",
      "Country 'KEN' complete in 44.74 seconds.\n",
      "\n",
      "\n",
      "Starting ISO KGZ...\n",
      "Country 'KGZ' complete in 34.27 seconds.\n",
      "\n",
      "\n",
      "Starting ISO KHM...\n",
      "Country 'KHM' complete in 30.94 seconds.\n",
      "\n",
      "\n",
      "Starting ISO KIR...\n",
      "Country 'KIR' complete in 61.34 seconds.\n",
      "\n",
      "\n",
      "Starting ISO KNA...\n",
      "Country 'KNA' complete in 20.66 seconds.\n",
      "\n",
      "\n",
      "Starting ISO KOR...\n",
      "Country 'KOR' complete in 40.0 seconds.\n",
      "\n",
      "\n",
      "Starting ISO KWT...\n",
      "Country 'KWT' complete in 23.23 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LAO...\n",
      "Country 'LAO' complete in 46.48 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LBN...\n",
      "Country 'LBN' complete in 22.12 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LBR...\n",
      "Country 'LBR' complete in 31.41 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LBY...\n",
      "Country 'LBY' complete in 80.16 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LCA...\n",
      "Country 'LCA' complete in 20.79 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LIE...\n",
      "Country 'LIE' complete in 13.29 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LKA...\n",
      "Country 'LKA' complete in 30.95 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LSO...\n",
      "Country 'LSO' complete in 32.93 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LTU...\n",
      "Country 'LTU' complete in 24.44 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LUX...\n",
      "Country 'LUX' complete in 14.45 seconds.\n",
      "\n",
      "\n",
      "Starting ISO LVA...\n",
      "Country 'LVA' complete in 21.74 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MAC...\n",
      "Country 'MAC' complete in 18.55 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MAF...\n",
      "Country 'MAF' complete in 19.22 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MAR...\n",
      "Country 'MAR' complete in 54.78 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MCO...\n",
      "Country 'MCO' complete in 12.67 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MDA...\n",
      "Country 'MDA' complete in 28.98 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MDG...\n",
      "Country 'MDG' complete in 62.25 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MDV...\n",
      "Country 'MDV' complete in 38.77 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MEX...\n",
      "Country 'MEX' complete in 94.45 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MHL...\n",
      "Country 'MHL' complete in 44.67 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MKD...\n",
      "Country 'MKD' complete in 21.02 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MLI...\n",
      "Country 'MLI' complete in 68.87 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MLT...\n",
      "Country 'MLT' complete in 16.63 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MMR...\n",
      "Country 'MMR' complete in 84.66 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MNE...\n",
      "Country 'MNE' complete in 21.26 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MNG...\n",
      "Country 'MNG' complete in 75.48 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MNP...\n",
      "Country 'MNP' complete in 38.12 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MOZ...\n",
      "Country 'MOZ' complete in 68.71 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MRT...\n",
      "Country 'MRT' complete in 67.24 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MUS...\n",
      "Country 'MUS' complete in 49.09 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MWI...\n",
      "Country 'MWI' complete in 42.39 seconds.\n",
      "\n",
      "\n",
      "Starting ISO MYS...\n",
      "Country 'MYS' complete in 37.47 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NAM...\n",
      "Country 'NAM' complete in 60.78 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NCL...\n",
      "Country 'NCL' complete in 33.8 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NER...\n",
      "Country 'NER' complete in 60.57 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NGA...\n",
      "Country 'NGA' complete in 47.17 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NIC...\n",
      "Country 'NIC' complete in 32.11 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NLD...\n",
      "Country 'NLD' complete in 27.86 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NPL...\n",
      "Country 'NPL' complete in 33.79 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NRU...\n",
      "Country 'NRU' complete in 22.5 seconds.\n",
      "\n",
      "\n",
      "Starting ISO NZL...\n",
      "Country 'NZL' complete in 48.05 seconds.\n",
      "\n",
      "\n",
      "Starting ISO OMN...\n",
      "Country 'OMN' complete in 57.75 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PAK...\n",
      "Country 'PAK' complete in 79.18 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PAN...\n",
      "Country 'PAN' complete in 28.03 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PER...\n",
      "Country 'PER' complete in 71.99 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PHL...\n",
      "Country 'PHL' complete in 67.61 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PLW...\n",
      "Country 'PLW' complete in 24.24 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PNG...\n",
      "Country 'PNG' complete in 51.15 seconds.\n",
      "\n",
      "\n",
      "Starting ISO POL...\n",
      "Country 'POL' complete in 48.27 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PRI...\n",
      "Country 'PRI' complete in 21.21 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PRK...\n",
      "Country 'PRK' complete in 39.23 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PRT...\n",
      "Country 'PRT' complete in 69.16 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PRY...\n",
      "Country 'PRY' complete in 47.1 seconds.\n",
      "\n",
      "\n",
      "Starting ISO PYF...\n",
      "Country 'PYF' complete in 74.31 seconds.\n",
      "\n",
      "\n",
      "Starting ISO QAT...\n",
      "Country 'QAT' complete in 24.3 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ROU...\n",
      "Country 'ROU' complete in 38.34 seconds.\n",
      "\n",
      "\n",
      "Starting ISO RUS...\n",
      "Country 'RUS' complete in 327.86 seconds.\n",
      "\n",
      "\n",
      "Starting ISO RWA...\n",
      "Country 'RWA' complete in 26.72 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SAU...\n",
      "Country 'SAU' complete in 82.85 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SDN...\n",
      "Country 'SDN' complete in 61.8 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SEN...\n",
      "Country 'SEN' complete in 31.77 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SGP...\n",
      "Country 'SGP' complete in 21.61 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SLB...\n",
      "Country 'SLB' complete in 37.28 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SLE...\n",
      "Country 'SLE' complete in 31.45 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SLV...\n",
      "Country 'SLV' complete in 23.79 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SMR...\n",
      "Country 'SMR' complete in 12.6 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SOM...\n",
      "Country 'SOM' complete in 56.3 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SRB...\n",
      "Country 'SRB' complete in 33.14 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SSD...\n",
      "Country 'SSD' complete in 44.09 seconds.\n",
      "\n",
      "\n",
      "Starting ISO STP...\n",
      "Country 'STP' complete in 25.88 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SUR...\n",
      "Country 'SUR' complete in 31.11 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SVK...\n",
      "Country 'SVK' complete in 22.17 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SVN...\n",
      "Country 'SVN' complete in 19.4 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SWE...\n",
      "Country 'SWE' complete in 102.63 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SWZ...\n",
      "Country 'SWZ' complete in 28.89 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SXM...\n",
      "Country 'SXM' complete in 19.44 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SYC...\n",
      "Country 'SYC' complete in 37.88 seconds.\n",
      "\n",
      "\n",
      "Starting ISO SYR...\n",
      "Country 'SYR' complete in 37.54 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TCA...\n",
      "Country 'TCA' complete in 20.32 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TCD...\n",
      "Country 'TCD' complete in 71.74 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TGO...\n",
      "Country 'TGO' complete in 33.42 seconds.\n",
      "\n",
      "\n",
      "Starting ISO THA...\n",
      "Country 'THA' complete in 63.24 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TJK...\n",
      "Country 'TJK' complete in 34.74 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TKM...\n",
      "Country 'TKM' complete in 51.96 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TLS...\n",
      "Country 'TLS' complete in 26.65 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TON...\n",
      "Country 'TON' complete in 41.74 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TTO...\n",
      "Country 'TTO' complete in 23.95 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TUN...\n",
      "Country 'TUN' complete in 48.19 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TUR...\n",
      "Country 'TUR' complete in 45.37 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TUV...\n",
      "Country 'TUV' complete in 26.36 seconds.\n",
      "\n",
      "\n",
      "Starting ISO TZA...\n",
      "Country 'TZA' complete in 51.21 seconds.\n",
      "\n",
      "\n",
      "Starting ISO UGA...\n",
      "Country 'UGA' complete in 35.79 seconds.\n",
      "\n",
      "\n",
      "Starting ISO UKR...\n",
      "Country 'UKR' complete in 60.85 seconds.\n",
      "\n",
      "\n",
      "Starting ISO URY...\n",
      "Country 'URY' complete in 35.97 seconds.\n",
      "\n",
      "\n",
      "Starting ISO USA...\n",
      "Country 'USA' complete in 350.85 seconds.\n",
      "\n",
      "\n",
      "Starting ISO UZB...\n",
      "Country 'UZB' complete in 56.53 seconds.\n",
      "\n",
      "\n",
      "Starting ISO VCT...\n",
      "Country 'VCT' complete in 21.27 seconds.\n",
      "\n",
      "\n",
      "Starting ISO VEN...\n",
      "Country 'VEN' complete in 58.18 seconds.\n",
      "\n",
      "\n",
      "Starting ISO VGB...\n",
      "Country 'VGB' complete in 19.63 seconds.\n",
      "\n",
      "\n",
      "Starting ISO VIR...\n",
      "Country 'VIR' complete in 20.77 seconds.\n",
      "\n",
      "\n",
      "Starting ISO VNM...\n",
      "Country 'VNM' complete in 64.69 seconds.\n",
      "\n",
      "\n",
      "Starting ISO VUT...\n",
      "Country 'VUT' complete in 42.07 seconds.\n",
      "\n",
      "\n",
      "Starting ISO WSM...\n",
      "Country 'WSM' complete in 25.37 seconds.\n",
      "\n",
      "\n",
      "Starting ISO YEM...\n",
      "Country 'YEM' complete in 41.5 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ZAF...\n",
      "Country 'ZAF' complete in 62.93 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ZMB...\n",
      "Country 'ZMB' complete in 49.74 seconds.\n",
      "\n",
      "\n",
      "Starting ISO ZWE...\n",
      "Country 'ZWE' complete in 42.79 seconds.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MERGE IN LUC DATA TO KCC DATA\n",
    "\n",
    "import re\n",
    "importlib.reload(sf)\n",
    "\n",
    "    \n",
    "fp_coords = \"/Users/jsyme/Documents/Projects/FY21/SWCHE131_1000/Data/AFOLU/kcc_and_lndusecat_country/index_right_coords.csv\"\n",
    "\n",
    "# get files \n",
    "regex_match = re.compile(\"kcc_and_lndusecat_by_iso_(\\D*).csv\")\n",
    "dir_read = os.path.join(dir_data, \"kcc_cells_merged_to_country\")\n",
    "fls_read = sorted([x for x in os.listdir(dir_read) if regex_match.match(x) is not None])\n",
    "isos_avail = [regex_match.match(x).groups()[0] for x in fls_read]\n",
    "field_index = \"index_right\"\n",
    "\n",
    "# initialize output location for aggregates\n",
    "dir_output_aggs = os.path.join(dir_data, \"KCC_LUC_aggs_by_country\")\n",
    "os.makedirs(dir_output_aggs, exist_ok = True) if not os.path.exists(dir_output_aggs) else None\n",
    "\n",
    "fls_complete = []\n",
    "\n",
    "for i, fl in enumerate(fls_read):\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    iso = regex_match.match(fl).groups()[0]\n",
    "    print(f\"Starting ISO {iso}...\")\n",
    "    \n",
    "    fp_in_kcc = os.path.join(dir_read, fl)\n",
    "    fp_out_kcc_agg = os.path.join(dir_output_aggs, fl.replace(\".csv\", \"_agg.csv\"))\n",
    "    \n",
    "    # get data and rename\n",
    "    df_indexed_new = (\n",
    "        merge_gridded_data_to_kcc_file(\n",
    "            df_lu,\n",
    "            fp_coords,\n",
    "            fp_in_kcc,\n",
    "            field_luc,\n",
    "            output_as_index = True,\n",
    "        )\n",
    "        .rename(columns = {\"value\": field_kcc})\n",
    "    )\n",
    "    \n",
    "    \n",
    "    df_indexed_agg = (\n",
    "        sf.get_index_fields_count(\n",
    "            df_indexed_new,\n",
    "            fields_index = [field_kcc, field_luc]\n",
    "        )\n",
    "        .sort_values(by = [\"count\"], ascending = False)\n",
    "    )\n",
    "\n",
    "    # export files\n",
    "    df_indexed_new.to_csv(\n",
    "        fp_in_kcc,\n",
    "        index = None,\n",
    "        encoding = \"UTF-8\"\n",
    "    )\n",
    "    df_indexed_agg.to_csv(\n",
    "        fp_out_kcc_agg,\n",
    "        index = None,\n",
    "        encoding = \"UTF-8\"\n",
    "    )\n",
    "    \n",
    "    \n",
    "    t1 = time.time()\n",
    "    t_elapsed = sf.get_time_elapsed(t0)\n",
    "    fls_complete.append(fl)\n",
    "    print(f\"Country '{iso}' complete in {t_elapsed} seconds.\\n\\n\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "173f9186-35a9-48f5-b0d8-ec598a9d181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.concat([df_get[[\"index_right\"]], df_ord.drop([\"x\", \"y\"], axis = 1)], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "a8ca7d99-c179-4c47-9ef0-ce5e017ca58c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABW'"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regex_match.match(fls_read[0]).groups()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "b18a91de-ce89-45e6-b321-32007bfa66bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>kcc</th>\n",
       "      <th>code</th>\n",
       "      <th>name</th>\n",
       "      <th>wet_dry_cat</th>\n",
       "      <th>temperate_tropical_cat</th>\n",
       "      <th>description</th>\n",
       "      <th>group</th>\n",
       "      <th>precipitation_type</th>\n",
       "      <th>level_of_heat</th>\n",
       "      <th>ipcc_forest</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>110</td>\n",
       "      <td>Af</td>\n",
       "      <td>Tropical, rainforest</td>\n",
       "      <td>wet</td>\n",
       "      <td>tropical</td>\n",
       "      <td>Tropical rainforest climate</td>\n",
       "      <td>Tropical</td>\n",
       "      <td>Rainforest</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tropical rainforest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>Aw</td>\n",
       "      <td>Tropical, savannah</td>\n",
       "      <td>wet</td>\n",
       "      <td>tropical</td>\n",
       "      <td>Tropical savanna, wet</td>\n",
       "      <td>Tropical</td>\n",
       "      <td>Savanna, Wet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tropical shrublands</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>130</td>\n",
       "      <td>Am</td>\n",
       "      <td>Tropical, monsoon</td>\n",
       "      <td>wet</td>\n",
       "      <td>tropical</td>\n",
       "      <td>Tropical monsoon climate</td>\n",
       "      <td>Tropical</td>\n",
       "      <td>Monsoon</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Tropical moist deciduous forest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>211</td>\n",
       "      <td>BWh</td>\n",
       "      <td>Arid, desert, hot</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Hot deserts climate</td>\n",
       "      <td>Arid</td>\n",
       "      <td>Desert</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Desert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>212</td>\n",
       "      <td>BWk</td>\n",
       "      <td>Arid, desert, cold</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Cold desert climate</td>\n",
       "      <td>Arid</td>\n",
       "      <td>Desert</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Desert</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>221</td>\n",
       "      <td>BSh</td>\n",
       "      <td>Arid, steppe, hot</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Hot semi-arid (steppe) climate</td>\n",
       "      <td>Arid</td>\n",
       "      <td>Steppe</td>\n",
       "      <td>Hot</td>\n",
       "      <td>Steppe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>222</td>\n",
       "      <td>BSk</td>\n",
       "      <td>Arid, steppe, cold</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Cold semi-arid (steppe) climate</td>\n",
       "      <td>Arid</td>\n",
       "      <td>Steppe</td>\n",
       "      <td>Cold</td>\n",
       "      <td>Steppe</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>311</td>\n",
       "      <td>Csa</td>\n",
       "      <td>Temperate, dry summer, hot summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Hot-summer Mediterranean climate</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Dry summer</td>\n",
       "      <td>Hot summer</td>\n",
       "      <td>Sub-tropical dry forests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>312</td>\n",
       "      <td>Csb</td>\n",
       "      <td>Temperate, dry summer, warm summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Warm-summer Mediterranean climate</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Dry summer</td>\n",
       "      <td>Warm summer</td>\n",
       "      <td>Sub-tropical dry forests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>313</td>\n",
       "      <td>Csc</td>\n",
       "      <td>Temperate, dry summer, cold summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Cool-summer Mediterranean climate</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Dry summer</td>\n",
       "      <td>Cold summer</td>\n",
       "      <td>Sub-tropical dry forests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>321</td>\n",
       "      <td>Cwa</td>\n",
       "      <td>Temperate, dry winter, hot summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Monsoon-influenced humid subtropical climate</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Dry winter</td>\n",
       "      <td>Hot summer</td>\n",
       "      <td>Sub-tropical humid forests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>322</td>\n",
       "      <td>Cwb</td>\n",
       "      <td>Temperate, dry winter, warm summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Subtropical highland climate or temperate ocea...</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Dry winter</td>\n",
       "      <td>Warm summer</td>\n",
       "      <td>Sub-tropical mountain systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>323</td>\n",
       "      <td>Cwc</td>\n",
       "      <td>Temperate, dry winter, cold summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Cold subtropical highland climate or subpolar ...</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Dry winter</td>\n",
       "      <td>Cold summer</td>\n",
       "      <td>Sub-tropical mountain systems</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>331</td>\n",
       "      <td>Cfa</td>\n",
       "      <td>Temperate, without dry season, hot summer</td>\n",
       "      <td>wet</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Humid subtropical climate</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Without dry season</td>\n",
       "      <td>Hot summer</td>\n",
       "      <td>Sub-tropical humid forests</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>332</td>\n",
       "      <td>Cfb</td>\n",
       "      <td>Temperate, without dry season, warm summer</td>\n",
       "      <td>wet</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Temperate oceanic climate</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Without dry season</td>\n",
       "      <td>Warm summer</td>\n",
       "      <td>Oceanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>333</td>\n",
       "      <td>Cfc</td>\n",
       "      <td>Temperate, without dry season, cold summer</td>\n",
       "      <td>wet</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Subpolar oceanic climate</td>\n",
       "      <td>Temperate</td>\n",
       "      <td>Without dry season</td>\n",
       "      <td>Cold summer</td>\n",
       "      <td>Oceanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>411</td>\n",
       "      <td>Dsa</td>\n",
       "      <td>Boreal, dry summer, hot summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Hot, dry-summer continental climate</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Dry summer</td>\n",
       "      <td>Hot summer</td>\n",
       "      <td>Continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>412</td>\n",
       "      <td>Dsb</td>\n",
       "      <td>Boreal, dry summer, warm summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Warm, dry-summer continental climate</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Dry summer</td>\n",
       "      <td>Warm summer</td>\n",
       "      <td>Continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>413</td>\n",
       "      <td>Dsc</td>\n",
       "      <td>Boreal, dry summer, cold summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Dry-summer subarctic climate</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Dry summer</td>\n",
       "      <td>Cold summer</td>\n",
       "      <td>Coniferous\\nTundra woodland\\nMountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>414</td>\n",
       "      <td>Dsd</td>\n",
       "      <td>Boreal, dry summer, very cold winter</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coniferous\\nTundra woodland\\nMountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>421</td>\n",
       "      <td>Dwa</td>\n",
       "      <td>Boreal, dry winter, hot summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Monsoon-influenced hot-summer humid continenta...</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Dry winter</td>\n",
       "      <td>Hot summer</td>\n",
       "      <td>Continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>422</td>\n",
       "      <td>Dwb</td>\n",
       "      <td>Boreal, dry winter, warm summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Monsoon-influenced warm-summer humid continent...</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Dry winter</td>\n",
       "      <td>Warm summer</td>\n",
       "      <td>Continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>423</td>\n",
       "      <td>Dwc</td>\n",
       "      <td>Boreal, dry winter, cold summer</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Monsoon-influenced subarctic climate</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Dry winter</td>\n",
       "      <td>Cold summer</td>\n",
       "      <td>Coniferous\\nTundra woodland\\nMountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>424</td>\n",
       "      <td>Dwd</td>\n",
       "      <td>Boreal, dry winter, very cold winter</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Monsoon-influenced extremely cold subarctic cl...</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Dry winter</td>\n",
       "      <td>Very cold winter</td>\n",
       "      <td>Coniferous\\nTundra woodland\\nMountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>431</td>\n",
       "      <td>Dfa</td>\n",
       "      <td>Boreal, without dry season, hot summer</td>\n",
       "      <td>wet</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Hot-summer humid continental climate</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Without dry season</td>\n",
       "      <td>Hot summer</td>\n",
       "      <td>Continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>432</td>\n",
       "      <td>Dfb</td>\n",
       "      <td>Boreal, without dry season, warm summer</td>\n",
       "      <td>wet</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Warm-summer humid continental climate</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Without dry season</td>\n",
       "      <td>Warm summer</td>\n",
       "      <td>Continental</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>433</td>\n",
       "      <td>Dfc</td>\n",
       "      <td>Boreal, without dry season, cold summer</td>\n",
       "      <td>wet</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Subarctic climate</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Without dry season</td>\n",
       "      <td>Cold summer</td>\n",
       "      <td>Coniferous\\nTundra woodland\\nMountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>434</td>\n",
       "      <td>Dfd</td>\n",
       "      <td>Boreal, without dry season, very cold winter</td>\n",
       "      <td>wet</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Extremely cold subarctic climate</td>\n",
       "      <td>Cold (continental)</td>\n",
       "      <td>Without dry season</td>\n",
       "      <td>Very cold winter</td>\n",
       "      <td>Coniferous\\nTundra woodland\\nMountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>510</td>\n",
       "      <td>ET</td>\n",
       "      <td>Polar, tundra</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Tundra</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coniferous\\nTundra woodland\\nMountain</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>520</td>\n",
       "      <td>EF</td>\n",
       "      <td>Polar, frost</td>\n",
       "      <td>dry</td>\n",
       "      <td>temperate</td>\n",
       "      <td>Ice cap climate</td>\n",
       "      <td>Polar</td>\n",
       "      <td>Ice cap</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Coniferous\\nTundra woodland\\nMountain</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    kcc code                                          name wet_dry_cat  \\\n",
       "0   110   Af                          Tropical, rainforest         wet   \n",
       "1   120   Aw                            Tropical, savannah         wet   \n",
       "2   130   Am                             Tropical, monsoon         wet   \n",
       "3   211  BWh                             Arid, desert, hot         dry   \n",
       "4   212  BWk                            Arid, desert, cold         dry   \n",
       "5   221  BSh                             Arid, steppe, hot         dry   \n",
       "6   222  BSk                            Arid, steppe, cold         dry   \n",
       "7   311  Csa             Temperate, dry summer, hot summer         dry   \n",
       "8   312  Csb            Temperate, dry summer, warm summer         dry   \n",
       "9   313  Csc            Temperate, dry summer, cold summer         dry   \n",
       "10  321  Cwa             Temperate, dry winter, hot summer         dry   \n",
       "11  322  Cwb            Temperate, dry winter, warm summer         dry   \n",
       "12  323  Cwc            Temperate, dry winter, cold summer         dry   \n",
       "13  331  Cfa     Temperate, without dry season, hot summer         wet   \n",
       "14  332  Cfb    Temperate, without dry season, warm summer         wet   \n",
       "15  333  Cfc    Temperate, without dry season, cold summer         wet   \n",
       "16  411  Dsa                Boreal, dry summer, hot summer         dry   \n",
       "17  412  Dsb               Boreal, dry summer, warm summer         dry   \n",
       "18  413  Dsc               Boreal, dry summer, cold summer         dry   \n",
       "19  414  Dsd          Boreal, dry summer, very cold winter         dry   \n",
       "20  421  Dwa                Boreal, dry winter, hot summer         dry   \n",
       "21  422  Dwb               Boreal, dry winter, warm summer         dry   \n",
       "22  423  Dwc               Boreal, dry winter, cold summer         dry   \n",
       "23  424  Dwd          Boreal, dry winter, very cold winter         dry   \n",
       "24  431  Dfa        Boreal, without dry season, hot summer         wet   \n",
       "25  432  Dfb       Boreal, without dry season, warm summer         wet   \n",
       "26  433  Dfc       Boreal, without dry season, cold summer         wet   \n",
       "27  434  Dfd  Boreal, without dry season, very cold winter         wet   \n",
       "28  510   ET                                 Polar, tundra         dry   \n",
       "29  520   EF                                  Polar, frost         dry   \n",
       "\n",
       "   temperate_tropical_cat                                        description  \\\n",
       "0                tropical                        Tropical rainforest climate   \n",
       "1                tropical                              Tropical savanna, wet   \n",
       "2                tropical                           Tropical monsoon climate   \n",
       "3               temperate                                Hot deserts climate   \n",
       "4               temperate                                Cold desert climate   \n",
       "5               temperate                     Hot semi-arid (steppe) climate   \n",
       "6               temperate                    Cold semi-arid (steppe) climate   \n",
       "7               temperate                   Hot-summer Mediterranean climate   \n",
       "8               temperate                  Warm-summer Mediterranean climate   \n",
       "9               temperate                  Cool-summer Mediterranean climate   \n",
       "10              temperate       Monsoon-influenced humid subtropical climate   \n",
       "11              temperate  Subtropical highland climate or temperate ocea...   \n",
       "12              temperate  Cold subtropical highland climate or subpolar ...   \n",
       "13              temperate                          Humid subtropical climate   \n",
       "14              temperate                          Temperate oceanic climate   \n",
       "15              temperate                           Subpolar oceanic climate   \n",
       "16              temperate                Hot, dry-summer continental climate   \n",
       "17              temperate               Warm, dry-summer continental climate   \n",
       "18              temperate                       Dry-summer subarctic climate   \n",
       "19              temperate                                                NaN   \n",
       "20              temperate  Monsoon-influenced hot-summer humid continenta...   \n",
       "21              temperate  Monsoon-influenced warm-summer humid continent...   \n",
       "22              temperate               Monsoon-influenced subarctic climate   \n",
       "23              temperate  Monsoon-influenced extremely cold subarctic cl...   \n",
       "24              temperate               Hot-summer humid continental climate   \n",
       "25              temperate              Warm-summer humid continental climate   \n",
       "26              temperate                                  Subarctic climate   \n",
       "27              temperate                   Extremely cold subarctic climate   \n",
       "28              temperate                                             Tundra   \n",
       "29              temperate                                    Ice cap climate   \n",
       "\n",
       "                 group  precipitation_type     level_of_heat  \\\n",
       "0             Tropical          Rainforest               NaN   \n",
       "1             Tropical        Savanna, Wet               NaN   \n",
       "2             Tropical             Monsoon               NaN   \n",
       "3                 Arid              Desert               Hot   \n",
       "4                 Arid              Desert              Cold   \n",
       "5                 Arid              Steppe               Hot   \n",
       "6                 Arid              Steppe              Cold   \n",
       "7            Temperate          Dry summer        Hot summer   \n",
       "8            Temperate          Dry summer       Warm summer   \n",
       "9            Temperate          Dry summer       Cold summer   \n",
       "10           Temperate          Dry winter        Hot summer   \n",
       "11           Temperate          Dry winter       Warm summer   \n",
       "12           Temperate          Dry winter       Cold summer   \n",
       "13           Temperate  Without dry season        Hot summer   \n",
       "14           Temperate  Without dry season       Warm summer   \n",
       "15           Temperate  Without dry season       Cold summer   \n",
       "16  Cold (continental)          Dry summer        Hot summer   \n",
       "17  Cold (continental)          Dry summer       Warm summer   \n",
       "18  Cold (continental)          Dry summer       Cold summer   \n",
       "19                 NaN                 NaN               NaN   \n",
       "20  Cold (continental)          Dry winter        Hot summer   \n",
       "21  Cold (continental)          Dry winter       Warm summer   \n",
       "22  Cold (continental)          Dry winter       Cold summer   \n",
       "23  Cold (continental)          Dry winter  Very cold winter   \n",
       "24  Cold (continental)  Without dry season        Hot summer   \n",
       "25  Cold (continental)  Without dry season       Warm summer   \n",
       "26  Cold (continental)  Without dry season       Cold summer   \n",
       "27  Cold (continental)  Without dry season  Very cold winter   \n",
       "28                 NaN                 NaN               NaN   \n",
       "29               Polar             Ice cap               NaN   \n",
       "\n",
       "                              ipcc_forest  \n",
       "0                     Tropical rainforest  \n",
       "1                     Tropical shrublands  \n",
       "2         Tropical moist deciduous forest  \n",
       "3                                  Desert  \n",
       "4                                  Desert  \n",
       "5                                  Steppe  \n",
       "6                                  Steppe  \n",
       "7                Sub-tropical dry forests  \n",
       "8                Sub-tropical dry forests  \n",
       "9                Sub-tropical dry forests  \n",
       "10             Sub-tropical humid forests  \n",
       "11          Sub-tropical mountain systems  \n",
       "12          Sub-tropical mountain systems  \n",
       "13             Sub-tropical humid forests  \n",
       "14                                Oceanic  \n",
       "15                                Oceanic  \n",
       "16                            Continental  \n",
       "17                            Continental  \n",
       "18  Coniferous\\nTundra woodland\\nMountain  \n",
       "19  Coniferous\\nTundra woodland\\nMountain  \n",
       "20                            Continental  \n",
       "21                            Continental  \n",
       "22  Coniferous\\nTundra woodland\\nMountain  \n",
       "23  Coniferous\\nTundra woodland\\nMountain  \n",
       "24                            Continental  \n",
       "25                            Continental  \n",
       "26  Coniferous\\nTundra woodland\\nMountain  \n",
       "27  Coniferous\\nTundra woodland\\nMountain  \n",
       "28  Coniferous\\nTundra woodland\\nMountain  \n",
       "29  Coniferous\\nTundra woodland\\nMountain  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "attr_kcc.table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7992a9d6-7459-47d7-8ed3-d1d8d6573fcd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "64370c21-cbf6-4ea5-af16-2cc1046056cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>value</th>\n",
       "      <th>luc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>62775</td>\n",
       "      <td>120.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34199</td>\n",
       "      <td>120.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>30992</td>\n",
       "      <td>221.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>24622</td>\n",
       "      <td>221.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>15554</td>\n",
       "      <td>120.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>15117</td>\n",
       "      <td>120.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>9318</td>\n",
       "      <td>211.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>8390</td>\n",
       "      <td>221.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>7772</td>\n",
       "      <td>130.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>7344</td>\n",
       "      <td>221.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>4727</td>\n",
       "      <td>221.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4490</td>\n",
       "      <td>130.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>2586</td>\n",
       "      <td>221.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2028</td>\n",
       "      <td>120.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1632</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1411</td>\n",
       "      <td>322.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1275</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1052</td>\n",
       "      <td>130.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>895</td>\n",
       "      <td>211.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>794</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>717</td>\n",
       "      <td>321.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>714</td>\n",
       "      <td>130.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>660</td>\n",
       "      <td>110.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>638</td>\n",
       "      <td>120.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>565</td>\n",
       "      <td>120.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>467</td>\n",
       "      <td>120.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>439</td>\n",
       "      <td>211.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>421</td>\n",
       "      <td>120.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>383</td>\n",
       "      <td>120.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>280</td>\n",
       "      <td>221.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>238</td>\n",
       "      <td>211.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>203</td>\n",
       "      <td>321.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>194</td>\n",
       "      <td>221.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>192</td>\n",
       "      <td>130.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>186</td>\n",
       "      <td>130.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>152</td>\n",
       "      <td>322.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>135</td>\n",
       "      <td>211.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>102</td>\n",
       "      <td>130.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>78</td>\n",
       "      <td>211.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76</td>\n",
       "      <td>110.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>72</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>52</td>\n",
       "      <td>211.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>27</td>\n",
       "      <td>321.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>22</td>\n",
       "      <td>211.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>14</td>\n",
       "      <td>120.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>13</td>\n",
       "      <td>322.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11</td>\n",
       "      <td>211.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>5</td>\n",
       "      <td>312.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>110.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>5</td>\n",
       "      <td>311.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>110.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>130.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>130.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>2</td>\n",
       "      <td>321.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>211.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>312.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    count  value  luc\n",
       "8   62775  120.0    2\n",
       "10  34199  120.0    4\n",
       "38  30992  221.0    2\n",
       "39  24622  221.0    3\n",
       "9   15554  120.0    3\n",
       "11  15117  120.0    5\n",
       "35   9318  211.0    9\n",
       "40   8390  221.0    4\n",
       "21   7772  130.0    4\n",
       "42   7344  221.0    8\n",
       "41   4727  221.0    5\n",
       "19   4490  130.0    2\n",
       "43   2586  221.0    9\n",
       "16   2028  120.0   11\n",
       "2    1632  110.0    4\n",
       "54   1411  322.0    5\n",
       "52   1275  322.0    2\n",
       "26   1052  130.0   11\n",
       "34    895  211.0    8\n",
       "0     794  110.0    2\n",
       "48    717  321.0    2\n",
       "22    714  130.0    5\n",
       "5     660  110.0   11\n",
       "12    638  120.0    6\n",
       "15    565  120.0    9\n",
       "14    467  120.0    8\n",
       "29    439  211.0    3\n",
       "13    421  120.0    7\n",
       "7     383  120.0    1\n",
       "44    280  221.0   11\n",
       "36    238  211.0   11\n",
       "50    203  321.0    5\n",
       "37    194  221.0    1\n",
       "24    192  130.0    7\n",
       "23    186  130.0    6\n",
       "53    152  322.0    4\n",
       "28    135  211.0    2\n",
       "18    102  130.0    1\n",
       "31     78  211.0    5\n",
       "4      76  110.0    7\n",
       "20     72  130.0    3\n",
       "32     52  211.0    6\n",
       "49     27  321.0    4\n",
       "30     22  211.0    4\n",
       "6      14  120.0    0\n",
       "55     13  322.0   11\n",
       "27     11  211.0    1\n",
       "47      5  312.0    5\n",
       "1       5  110.0    3\n",
       "45      5  311.0    2\n",
       "3       3  110.0    5\n",
       "25      2  130.0    9\n",
       "17      2  130.0    0\n",
       "51      2  321.0    9\n",
       "33      1  211.0    7\n",
       "46      1  312.0    2"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a97b9-471d-4780-b4cd-51e7071dbccf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbfce8c0-8bb1-40f8-849a-5ab321b3623f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "dd97c372-59e8-4ecb-a253-dba356a07404",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "372eaf8d-405c-4248-9554-e7740f39447f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0mget_gridded_data_by_index_data_frame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdf_grid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdf_index_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdigits_round_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfield_lat\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'y'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfield_lon\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfield_value_index\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'value'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfield_value_index_out\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'kcc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfield_value_new\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'luc'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmissing_val\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m999\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mreturn_df\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Using broken (by country) KCC data frames, get land use categories by\n",
       "    cell centroid (assumes KCC and LU grids have same centroids)\n",
       "    \n",
       "\n",
       "Function Arguments\n",
       "------------------\n",
       "- df_grid: DataFrame containing lat as row index and lon as column\n",
       "    index. Values in df_index_data[field_lat] and df_index_data[field_lon] are \n",
       "    matched to y and x indices, respectively, rounding to match using\n",
       "    digits_round_index.\n",
       "- df_index_data: input DataFrame containing KCC indices by lat/lon centroid, \n",
       "    including x, y, and value\n",
       "    \n",
       "\n",
       "Keyword Arguments\n",
       "-----------------\n",
       "- digits_round_index: number of digits to use for rounding lat/lon to look\n",
       "    values in land use\n",
       "- field_lat: field containing latitude\n",
       "- field_lon: field_containing longitude\n",
       "- field_value_index: field in df_index_data containing data values\n",
       "- field_value_index_out: new field name for data field stored in df_index_data\n",
       "- field_value_new: new field name for data merged in from df_grid\n",
       "- missing_val: value to use if lat/lon not found in df_grid\n",
       "- return_df: return a dataframe merging the new column to df_index_data? if False,\n",
       "    returns ordered column vector only\n",
       "\u001b[0;31mFile:\u001b[0m      /var/folders/8m/3ll2cn6d1hdcs6gjqxr2jx5d2hffc9/T/ipykernel_30101/3582860588.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?get_gridded_data_by_index_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "1995c21b-65c7-40a2-a310-86b798a3d7db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>value</th>\n",
       "      <th>luc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>243588</th>\n",
       "      <td>-87.120833</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>120.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243589</th>\n",
       "      <td>-87.112500</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>120.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243590</th>\n",
       "      <td>-87.104167</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>120.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243591</th>\n",
       "      <td>-87.095833</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>120.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>243592</th>\n",
       "      <td>-87.087500</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>120.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9269</th>\n",
       "      <td>125.054167</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9270</th>\n",
       "      <td>125.062500</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9271</th>\n",
       "      <td>125.070833</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9272</th>\n",
       "      <td>125.079167</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9273</th>\n",
       "      <td>125.087500</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244050 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 x          y  value  luc\n",
       "243588  -87.120833  12.420833  120.0   11\n",
       "243589  -87.112500  12.420833  120.0   11\n",
       "243590  -87.104167  12.420833  120.0    7\n",
       "243591  -87.095833  12.420833  120.0    7\n",
       "243592  -87.087500  12.420833  120.0    7\n",
       "...            ...        ...    ...  ...\n",
       "9269    125.054167  12.629167  110.0   11\n",
       "9270    125.062500  12.629167  110.0    4\n",
       "9271    125.070833  12.629167  110.0    2\n",
       "9272    125.079167  12.629167  110.0   11\n",
       "9273    125.087500  12.629167  110.0   11\n",
       "\n",
       "[244050 rows x 4 columns]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a51e6d-1498-4f7a-a5d5-c47892772d77",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.round(np.array(df_lat_lon[\"x\"]), decimals = 6)\n",
    "y = np.round(np.array(df_lat_lon[\"y\"]), decimals = 6)\n",
    "\n",
    "t0 = time.time()\n",
    "vec = df_lu.loc[y, x]\n",
    "t1 = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1e76cdc3-1ecb-478b-aa56-4eee158c304e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>value</th>\n",
       "      <th>luc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-70.054167</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>221.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-70.045833</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>221.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-61.412500</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-61.404167</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-61.395833</td>\n",
       "      <td>12.629167</td>\n",
       "      <td>110.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244045</th>\n",
       "      <td>-71.479167</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>211.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244046</th>\n",
       "      <td>-71.470833</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>211.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244047</th>\n",
       "      <td>-69.904167</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>221.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244048</th>\n",
       "      <td>-69.895833</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>221.0</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>244049</th>\n",
       "      <td>-69.887500</td>\n",
       "      <td>12.420833</td>\n",
       "      <td>221.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>244050 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                x          y  value  luc\n",
       "0      -70.054167  12.629167  221.0   11\n",
       "1      -70.045833  12.629167  221.0   11\n",
       "2      -61.412500  12.629167  110.0   11\n",
       "3      -61.404167  12.629167  110.0   11\n",
       "4      -61.395833  12.629167  110.0   11\n",
       "...           ...        ...    ...  ...\n",
       "244045 -71.479167  12.420833  211.0   11\n",
       "244046 -71.470833  12.420833  211.0   11\n",
       "244047 -69.904167  12.420833  221.0   11\n",
       "244048 -69.895833  12.420833  221.0   11\n",
       "244049 -69.887500  12.420833  221.0    3\n",
       "\n",
       "[244050 rows x 4 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "id": "dfc75de5-a1c1-4073-a79e-d680b5b51548",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = None\n",
    "for i, row in df_lat_lon.iterrows():    \n",
    "    r = row if r is None else r\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "id": "96959fc0-9150-40fa-90e2-8e745a05cb48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 493,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "digits_round_index = 6\n",
    "field_lat = \"y\"\n",
    "field_lon = \"x\"\n",
    "lat = np.round(float(r[field_lat]), decimals = digits_round_index)\n",
    "lon = np.round(float(r[field_lon]), decimals = digits_round_index)\n",
    "int(df_lu.loc[lat, lon])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "id": "7ac9c9f6-5fc1-4d12-a1e7-ce80a9af2755",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 488,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lu.loc[lat, lon]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "id": "d2e848a4-92e5-42a9-b09d-54e89c0387e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "147.36334896087646"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "df_try = pd.read_csv(\n",
    "    fp_coords,\n",
    "    header = None,\n",
    "    nrows = max_ind - min_ind + 1,\n",
    "    skiprows = min_ind + 1,\n",
    ")\n",
    "t1 = time.time()\n",
    "t1 - t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "id": "859acf15-1377-4825-a685-b74d11d21938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000001\n"
     ]
    }
   ],
   "source": [
    "\n",
    "arr = read_array(\n",
    "    fp_coords,\n",
    "    3,\n",
    "    10000000, \n",
    "    12000000,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "id": "6d466dba-209e-48c4-83cf-c90d7f3ff378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mSignature:\u001b[0m\n",
       "\u001b[0msf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_array_from_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mfp\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mn_cols\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmin_ind\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mmax_ind\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mdelim\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m','\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m    \u001b[0mskip_header\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
       "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "Read an array from a file. min_ind is first row, max_ind is last row + 1\n",
       "    (python style indexing). Only works with numeric values.\n",
       "    \n",
       "Reads like data frame index, so 0 would be the first row of data (unless\n",
       "    skip_header = False)\n",
       "    \n",
       "Keyword Arguments\n",
       "-----------------\n",
       "- delim: data deliminter\n",
       "- skip_header: skip the first row\n",
       "\u001b[0;31mFile:\u001b[0m      ~/Documents/Projects/git_jbus/lac_decarbonization/python/support_functions.py\n",
       "\u001b[0;31mType:\u001b[0m      function\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "?sf.read_array_from_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a935fadd-94bd-428d-a525-d2f4ef8d2622",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9d26a441-7d6b-48bd-a533-46b47a16e9ee",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "91dff60f-abdb-4c27-9df1-f3ebc1c6817f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fi.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "e046cb5e-82f6-4927-a854-01125ff3d873",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'-33.945833333333326,83.6375,510.0\\n'"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "?fi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329b1221-702c-4ab2-9231-6a89d8c2d163",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
